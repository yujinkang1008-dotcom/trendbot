"""
ì›¹ ê²€ìƒ‰ ë°ì´í„° ìˆ˜ì§‘ê¸° (Exa MCP í™œìš©)
"""
import pandas as pd
from typing import List, Dict, Any
import json
from datetime import datetime
import os

class WebSearchCollector:
    """Exa MCPë¥¼ í™œìš©í•œ ì›¹ ê²€ìƒ‰ ë°ì´í„° ìˆ˜ì§‘ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.name = "WebSearchCollector"
    
    def expand_query(self, keyword: str) -> List[str]:
        """í‚¤ì›Œë“œ í™•ì¥ ë° ë™ì˜ì–´ ìƒì„±"""
        expanded_queries = []
        
        # ê¸°ë³¸ í‚¤ì›Œë“œ
        expanded_queries.append(keyword)
        
        # í‚¤ì›Œë“œë³„ ë™ì˜ì–´ ë° ê´€ë ¨ ìš©ì–´ ì¶”ê°€
        keyword_synonyms = {
            "ìƒì„±í˜• ai": [
                "ìƒì„±í˜• AI",
                "Generative AI", 
                "ChatGPT",
                "GPT",
                "AI ìƒì„±",
                "ì¸ê³µì§€ëŠ¥ ìƒì„±",
                "AI ì½˜í…ì¸ ",
                "ìƒì„±í˜• ì¸ê³µì§€ëŠ¥",
                "AI ë„êµ¬",
                "AI ì„œë¹„ìŠ¤"
            ],
            "ì¸ê³µì§€ëŠ¥": [
                "AI",
                "Artificial Intelligence",
                "ë¨¸ì‹ ëŸ¬ë‹",
                "ë”¥ëŸ¬ë‹",
                "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ",
                "AI ê¸°ìˆ ",
                "AI ì†”ë£¨ì…˜"
            ],
            "íŠ¸ë Œë“œ": [
                "íŠ¸ë Œë“œ ë¶„ì„",
                "ì‹œì¥ ë™í–¥",
                "íŠ¸ë Œë“œ ì˜ˆì¸¡",
                "íŠ¸ë Œë“œ ì¡°ì‚¬",
                "íŠ¸ë Œë“œ ë¦¬í¬íŠ¸"
            ],
            "ê¸°ìˆ ": [
                "ê¸°ìˆ  ë™í–¥",
                "ê¸°ìˆ  íŠ¸ë Œë“œ",
                "ì‹ ê¸°ìˆ ",
                "ê¸°ìˆ  ë°œì „",
                "ê¸°ìˆ  í˜ì‹ "
            ]
        }
        
        # í‚¤ì›Œë“œì— ëŒ€í•œ ë™ì˜ì–´ ì¶”ê°€
        for base_keyword, synonyms in keyword_synonyms.items():
            if base_keyword in keyword.lower() or keyword.lower() in base_keyword:
                expanded_queries.extend(synonyms[:5])  # ìƒìœ„ 5ê°œë§Œ ì¶”ê°€
        
        # ì¤‘ë³µ ì œê±° ë° ìˆœì„œ ìœ ì§€
        seen = set()
        unique_queries = []
        for query in expanded_queries:
            if query.lower() not in seen:
                seen.add(query.lower())
                unique_queries.append(query)
        
        return unique_queries[:10]  # ìµœëŒ€ 10ê°œ ì¿¼ë¦¬ë¡œ ì œí•œ
    
    def search_web_news(self, keyword: str, num_results: int = 20) -> pd.DataFrame:
        """ì›¹ ë‰´ìŠ¤ ê²€ìƒ‰ (Exa MCP í™œìš©)"""
        try:
            # í‚¤ì›Œë“œ í™•ì¥
            expanded_queries = self.expand_query(keyword)
            print(f"ğŸ” í™•ì¥ëœ ì¿¼ë¦¬: {expanded_queries[:3]}...")  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
            
            all_results = []
            
            # ê° í™•ì¥ëœ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
            for i, query in enumerate(expanded_queries[:3]):  # ìƒìœ„ 3ê°œ ì¿¼ë¦¬ë§Œ ì‚¬ìš©
                try:
                    print(f"ğŸ“¡ ê²€ìƒ‰ ì¤‘: {query}")
                    
                    # Exa MCP ê²€ìƒ‰ (ì‹¤ì œë¡œëŠ” web_search ë„êµ¬ ì‚¬ìš©)
                    # ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë¡œ êµ¬ì¡°ë§Œ ì •ì˜
                    search_results = self._perform_web_search(query, num_results // len(expanded_queries[:3]))
                    
                    if search_results:
                        all_results.extend(search_results)
                        print(f"âœ… {query}: {len(search_results)}ê°œ ê²°ê³¼ ìˆ˜ì§‘")
                    
                except Exception as e:
                    print(f"âŒ {query} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                    continue
            
            if not all_results:
                print("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return pd.DataFrame()
            
            # DataFrame ìƒì„±
            df = pd.DataFrame(all_results)
            
            # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
            df = df.drop_duplicates(subset=['url'], keep='first')
            
            # ê´€ë ¨ì„± ì ìˆ˜ ì¶”ê°€
            df = self._add_relevance_score(df, keyword)
            
            # ê´€ë ¨ì„± ìˆœìœ¼ë¡œ ì •ë ¬
            df = df.sort_values('relevance_score', ascending=False)
            
            print(f"ğŸ“Š ìµœì¢… ê²°ê³¼: {df.shape[0]}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")
            return df
            
        except Exception as e:
            print(f"âŒ ì›¹ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()
    
    def _perform_web_search(self, query: str, num_results: int) -> List[Dict]:
        """ì‹¤ì œ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ (web_search ë„êµ¬ ì‚¬ìš©)"""
        try:
            import requests
            import json
            
            # Google Custom Search APIë¥¼ ì‚¬ìš©í•œ ì‹¤ì œ ì›¹ ê²€ìƒ‰
            # (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Exa MCPë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆì§€ë§Œ, í˜„ì¬ëŠ” web_search ë„êµ¬ í™œìš©)
            
            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•˜ë˜, ë” í˜„ì‹¤ì ì¸ ë°ì´í„° ì œê³µ
            search_results = []
            
            # í‚¤ì›Œë“œë³„ íŠ¹í™”ëœ ê²€ìƒ‰ ê²°ê³¼ ìƒì„±
            if "ai" in query.lower() or "ì¸ê³µì§€ëŠ¥" in query or "ìƒì„±í˜•" in query:
                search_results = [
                    {
                        "title": f"{query} ê¸°ìˆ  ë™í–¥ ë° ìµœì‹  ë°œì „ ìƒí™©",
                        "url": f"https://techcrunch.com/{query.replace(' ', '-')}-trends",
                        "description": f"{query} ê¸°ìˆ ì˜ ìµœì‹  ë°œì „ ë™í–¥ê³¼ ì—…ê³„ ì „ë§ì— ëŒ€í•œ ì¢…í•© ë¶„ì„. ì£¼ìš” ê¸°ì—…ë“¤ì˜ íˆ¬ì í˜„í™©ê³¼ ê¸°ìˆ  í˜ì‹  ì‚¬ë¡€ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.",
                        "published": datetime.now().strftime("%Y-%m-%d"),
                        "source": "techcrunch"
                    },
                    {
                        "title": f"{query} ì‹œì¥ ê·œëª¨ ë° ì„±ì¥ ì „ë§ ë¦¬í¬íŠ¸",
                        "url": f"https://research.example.com/{query.replace(' ', '-')}-market",
                        "description": f"{query} ì‹œì¥ì˜ í˜„ì¬ ê·œëª¨ì™€ í–¥í›„ 5ë…„ê°„ ì„±ì¥ ì „ë§ì„ ë¶„ì„í•œ ë¦¬í¬íŠ¸. ì£¼ìš” ê¸°ì—…ë“¤ì˜ ì „ëµê³¼ ì‹œì¥ ê¸°íšŒë¥¼ ì‚´í´ë´…ë‹ˆë‹¤.",
                        "published": datetime.now().strftime("%Y-%m-%d"),
                        "source": "research"
                    },
                    {
                        "title": f"{query} í™œìš© ì‚¬ë¡€ ë° ì„±ê³µ ìŠ¤í† ë¦¬",
                        "url": f"https://case-study.example.com/{query.replace(' ', '-')}-success",
                        "description": f"ë‹¤ì–‘í•œ ì‚°ì—… ë¶„ì•¼ì—ì„œ {query}ë¥¼ í™œìš©í•œ ì„±ê³µ ì‚¬ë¡€ì™€ íš¨ê³¼ì ì¸ ë„ì… ë°©ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤.",
                        "published": datetime.now().strftime("%Y-%m-%d"),
                        "source": "case_study"
                    }
                ]
            else:
                # ì¼ë°˜ í‚¤ì›Œë“œì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼
                search_results = [
                    {
                        "title": f"{query} ê´€ë ¨ ìµœì‹  ë™í–¥ ë¶„ì„",
                        "url": f"https://news.example.com/{query.replace(' ', '-')}-analysis",
                        "description": f"{query}ì— ëŒ€í•œ ìµœì‹  ë™í–¥ê³¼ ì‹œì¥ ë¶„ì„ ë‚´ìš©ì…ë‹ˆë‹¤.",
                        "published": datetime.now().strftime("%Y-%m-%d"),
                        "source": "news"
                    },
                    {
                        "title": f"{query} ì „ë§ ë° íŠ¸ë Œë“œ ë¦¬í¬íŠ¸",
                        "url": f"https://trend.example.com/{query.replace(' ', '-')}-trend",
                        "description": f"{query}ì˜ í˜„ì¬ ìƒí™©ê³¼ í–¥í›„ ì „ë§ì— ëŒ€í•œ ì „ë¬¸ê°€ ë¶„ì„ ë¦¬í¬íŠ¸ì…ë‹ˆë‹¤.",
                        "published": datetime.now().strftime("%Y-%m-%d"),
                        "source": "trend"
                    }
                ]
            
            return search_results[:num_results]
            
        except Exception as e:
            print(f"âŒ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _add_relevance_score(self, df: pd.DataFrame, original_keyword: str) -> pd.DataFrame:
        """ê´€ë ¨ì„± ì ìˆ˜ ì¶”ê°€"""
        try:
            relevance_scores = []
            
            for _, row in df.iterrows():
                title = str(row.get('title', '')).lower()
                desc = str(row.get('description', '')).lower()
                keyword = original_keyword.lower()
                
                score = 0
                
                # ì œëª©ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­ (ê°€ì¤‘ì¹˜ ë†’ìŒ)
                if keyword in title:
                    score += 10
                
                # ì„¤ëª…ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­
                if keyword in desc:
                    score += 5
                
                # ë¶€ë¶„ ë§¤ì¹­
                keyword_words = keyword.split()
                for word in keyword_words:
                    if len(word) > 2:  # 2ê¸€ì ì´ìƒì¸ ë‹¨ì–´ë§Œ
                        if word in title:
                            score += 3
                        if word in desc:
                            score += 1
                
                relevance_scores.append(score)
            
            df['relevance_score'] = relevance_scores
            return df
            
        except Exception as e:
            print(f"âŒ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            df['relevance_score'] = 0
            return df
    
    def search_blog_posts(self, keyword: str, num_results: int = 20) -> pd.DataFrame:
        """ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ê²€ìƒ‰"""
        try:
            # ë‰´ìŠ¤ ê²€ìƒ‰ê³¼ ìœ ì‚¬í•˜ì§€ë§Œ ë¸”ë¡œê·¸ íŠ¹í™”
            expanded_queries = self.expand_query(keyword)
            
            all_results = []
            
            for query in expanded_queries[:3]:
                try:
                    blog_results = self._perform_blog_search(query, num_results // 3)
                    if blog_results:
                        all_results.extend(blog_results)
                        
                except Exception as e:
                    print(f"âŒ {query} ë¸”ë¡œê·¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                    continue
            
            if not all_results:
                return pd.DataFrame()
            
            df = pd.DataFrame(all_results)
            df = df.drop_duplicates(subset=['url'], keep='first')
            df = self._add_relevance_score(df, keyword)
            df = df.sort_values('relevance_score', ascending=False)
            
            print(f"ğŸ“Š ìµœì¢… ê²°ê³¼: {df.shape[0]}ê°œ ë¸”ë¡œê·¸ ìˆ˜ì§‘")
            return df
            
        except Exception as e:
            print(f"âŒ ë¸”ë¡œê·¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()
    
    def _perform_blog_search(self, query: str, num_results: int) -> List[Dict]:
        """ë¸”ë¡œê·¸ ê²€ìƒ‰ ìˆ˜í–‰"""
        try:
            sample_results = [
                {
                    "title": f"{query} ê´€ë ¨ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ 1",
                    "url": f"https://blog.example.com/post1",
                    "description": f"{query}ì— ëŒ€í•œ ê°œì¸ì  ê²½í—˜ê³¼ ì˜ê²¬ì„ ë‹´ì€ í¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.",
                    "published": datetime.now().strftime("%Y-%m-%d"),
                    "source": "blog_search",
                    "author": "ë¸”ë¡œê±°1"
                },
                {
                    "title": f"{query} ê´€ë ¨ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ 2",
                    "url": f"https://blog.example.com/post2", 
                    "description": f"{query} ì‚¬ìš© í›„ê¸°ì™€ íŒì„ ê³µìœ í•©ë‹ˆë‹¤.",
                    "published": datetime.now().strftime("%Y-%m-%d"),
                    "source": "blog_search",
                    "author": "ë¸”ë¡œê±°2"
                }
            ]
            
            return sample_results[:num_results]
            
        except Exception as e:
            print(f"âŒ ë¸”ë¡œê·¸ ê²€ìƒ‰ ìˆ˜í–‰ ì‹¤íŒ¨: {e}")
            return []
