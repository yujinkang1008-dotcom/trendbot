"""
Naver ë°ì´í„° ìˆ˜ì§‘ê¸° (ë‰´ìŠ¤, ë¸”ë¡œê·¸, DataLab)
"""
import os
import urllib.parse as up
import pandas as pd
import datetime as dt
import requests
from typing import List, Dict, Any

class NaverCollector:
    """Naver ë°ì´í„° ìˆ˜ì§‘ í´ë˜ìŠ¤"""
    
    def __init__(self, client_id: str, client_secret: str):
        self.client_id = client_id
        self.client_secret = client_secret
    
    def _headers(self, json=False):
        """API í˜¸ì¶œìš© í—¤ë” ìƒì„±"""
        h = {
            "X-Naver-Client-Id": self.client_id,
            "X-Naver-Client-Secret": self.client_secret
        }
        if json:
            h["Content-Type"] = "application/json"
        return h
    
    def search_news(self, query: str, display: int = 20) -> pd.DataFrame:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰"""
        return search_news(query, display)
    
    def search_blog(self, query: str, display: int = 20) -> pd.DataFrame:
        """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰"""
        return search_blog(query, display)

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ
NAVER_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_SECRET = os.getenv("NAVER_CLIENT_SECRET")

# STRICT_FETCH ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¡œë“œ, ê¸°ë³¸ê°’ False)
STRICT_FETCH = os.getenv("STRICT_FETCH", "False").lower() == "true"

def _headers(json=False):
    """API í˜¸ì¶œìš© í—¤ë” ìƒì„±"""
    h = {
        "X-Naver-Client-Id": NAVER_ID,
        "X-Naver-Client-Secret": NAVER_SECRET
    }
    if json:
        h["Content-Type"] = "application/json"
    return h

def _assert(df, cols, name="dataset", min_rows=3):
    """ë°ì´í„°í”„ë ˆì„ ê²€ì¦ ìœ í‹¸ë¦¬í‹°"""
    if df is None or len(df) < min_rows:
        raise ValueError(f"{name}: too few rows (expected >= {min_rows}, got {len(df) if df is not None else 0})")
    missing = [c for c in cols if c not in df.columns]
    if missing:
        raise ValueError(f"{name}: missing columns {missing}")

def search_news(query: str, display: int = 20) -> pd.DataFrame:
    """
    Naver ë‰´ìŠ¤ ê²€ìƒ‰ (GET + í—¤ë” 2ê°œ) + ë””ë²„ê¹…
    
    Args:
        query: ê²€ìƒ‰ í‚¤ì›Œë“œ
        display: ê²€ìƒ‰ ê²°ê³¼ ìˆ˜
        
    Returns:
        pd.DataFrame: ë‰´ìŠ¤ ë°ì´í„°
    """
    print(f"ğŸ” Naver ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘: '{query}', {display}ê°œ ê²°ê³¼")
    
    try:
        q = up.quote(query)
        url = f"https://openapi.naver.com/v1/search/news.json?query={q}&display={display}&start=1&sort=date"
        
        print(f"ğŸ“¡ API URL: {url}")
        
        response = requests.get(url, headers=_headers())
        response.raise_for_status()
        
        js = response.json()
        items = js.get("items", [])
        
        print(f"ğŸ“Š API ì‘ë‹µ: {len(items)}ê°œ ì•„ì´í…œ")
        
        if not items and STRICT_FETCH:
            raise ValueError(f"Naver ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {query}")
        
        # ë°ì´í„° í’ˆì§ˆ ê²€ì¦
        valid_items = []
        for i, it in enumerate(items):
            title = it.get("title", "")
            desc = it.get("description", "")
            
            # HTML íƒœê·¸ ì œê±° í™•ì¸
            clean_title = title.replace("<b>", "").replace("</b>", "").replace("&quot;", '"').replace("&amp;", "&")
            clean_desc = desc.replace("<b>", "").replace("</b>", "").replace("&quot;", '"').replace("&amp;", "&")
            
            if i < 3:  # ì²˜ìŒ 3ê°œ ì•„ì´í…œë§Œ ë””ë²„ê¹… ì¶œë ¥
                print(f"  ğŸ“° ì•„ì´í…œ {i+1}:")
                print(f"    ì œëª©: {clean_title[:50]}...")
                print(f"    ì„¤ëª…: {clean_desc[:50]}...")
            
            valid_items.append({
                "title": clean_title,
                "url": it.get("link", ""),
                "published": it.get("pubDate", ""),
                "desc": clean_desc
            })
        
        df = pd.DataFrame(valid_items)
        
        print(f"âœ… Naver ë‰´ìŠ¤ ë°ì´í„° ìƒì„±: {df.shape}")
        _assert(df, ["title", "url", "published"], "naver_news")
        return df
        
    except Exception as e:
        if STRICT_FETCH:
            raise
        print(f"âŒ Naver ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜ ({query}): {e}")
        return pd.DataFrame(columns=["title", "url", "published", "desc"])

def search_blog(query: str, display: int = 20) -> pd.DataFrame:
    """
    Naver ë¸”ë¡œê·¸ ê²€ìƒ‰ (GET + í—¤ë” 2ê°œ)
    
    Args:
        query: ê²€ìƒ‰ í‚¤ì›Œë“œ
        display: ê²€ìƒ‰ ê²°ê³¼ ìˆ˜
        
    Returns:
        pd.DataFrame: ë¸”ë¡œê·¸ ë°ì´í„°
    """
    try:
        q = up.quote(query)
        url = f"https://openapi.naver.com/v1/search/blog.json?query={q}&display={display}&start=1&sort=date"
        
        response = requests.get(url, headers=_headers())
        response.raise_for_status()
        
        js = response.json()
        items = js.get("items", [])
        
        if not items and STRICT_FETCH:
            raise ValueError(f"Naver ë¸”ë¡œê·¸ ê²€ìƒ‰ ì‹¤íŒ¨: {query}")
        
        df = pd.DataFrame([{
            "title": it.get("title", ""),
            "url": it.get("link", ""),
            "published": it.get("postdate", ""),
            "desc": it.get("description", ""),
            "bloggername": it.get("bloggername", "")
        } for it in items])
        
        _assert(df, ["title", "url", "published"], "naver_blog")
        return df
        
    except Exception as e:
        if STRICT_FETCH:
            raise
        print(f"Naver ë¸”ë¡œê·¸ ê²€ìƒ‰ ì˜¤ë¥˜ ({query}): {e}")
        return pd.DataFrame(columns=["title", "url", "published", "desc", "bloggername"])

def datalab_timeseries_multi(keywords: list, start: str, end: str, timeUnit: str = "date") -> pd.DataFrame:
    """
    Naver DataLab ë‹¤ì¤‘ í‚¤ì›Œë“œ ì‹œê³„ì—´ ë°ì´í„° ìˆ˜ì§‘ (POST + Content-Type: application/json + JSON ë°”ë””)
    
    Args:
        keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        start: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)
        end: ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)
        timeUnit: ì‹œê°„ ë‹¨ìœ„ (date, week, month)
        
    Returns:
        pd.DataFrame: ì‹œê³„ì—´ ë°ì´í„° (period + ê° í‚¤ì›Œë“œ ì»¬ëŸ¼)
        
    Raises:
        ValueError: ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ ë˜ëŠ” ë¹ˆ ê²°ê³¼
    """
    from src.common.trace import snapshot_df, log_shape
    
    try:
        print(f"Naver DataLab ë‹¤ì¤‘ ìˆ˜ì§‘ ì‹œì‘: {keywords}, ê¸°ê°„: {start} ~ {end}")
        
        # í‚¤ì›Œë“œ ê·¸ë£¹ ìƒì„±
        keyword_groups = []
        for keyword in keywords:
            keyword_groups.append({
                "groupName": keyword,
                "keywords": [keyword]
            })
        
        payload = {
            "startDate": start,
            "endDate": end,
            "timeUnit": timeUnit,
            "keywordGroups": keyword_groups,
            "device": "",
            "ages": [],
            "gender": ""
        }
        
        response = requests.post(
            "https://openapi.naver.com/v1/datalab/search",
            headers=_headers(json=True),
            json=payload
        )
        response.raise_for_status()
        
        data = response.json()
        results = data.get("results", [])
        
        if not results:
            raise ValueError(f"Naver DataLab: ë¹ˆ ê²°ê³¼ {keywords}")
        
        # ê²°ê³¼ë¥¼ wide í˜•íƒœë¡œ ë¨¸ì§€
        merged_df = None
        for result in results:
            keyword_name = result["title"]
            keyword_data = pd.DataFrame(result["data"])
            
            if keyword_data.empty:
                print(f"âš ï¸ {keyword_name}: ë¹ˆ ë°ì´í„°")
                continue
                
            keyword_data = keyword_data.rename(columns={"ratio": keyword_name})
            keyword_data = keyword_data.set_index("period")
            
            if merged_df is None:
                merged_df = keyword_data
            else:
                merged_df = merged_df.join(keyword_data, how='outer')
        
        if merged_df is None or merged_df.empty:
            raise ValueError(f"Naver DataLab: ìµœì¢… ë¹ˆ ê²°ê³¼ {keywords}")
        
        # ì¸ë±ìŠ¤ë¥¼ ë‹¤ì‹œ ì»¬ëŸ¼ìœ¼ë¡œ
        merged_df = merged_df.reset_index()
        
        # í‚¤ì›Œë“œ ì»¬ëŸ¼ í™•ì¸
        missing_keywords = [kw for kw in keywords if kw not in merged_df.columns]
        if missing_keywords:
            raise ValueError(f"Naver DataLab: ëˆ„ë½ëœ í‚¤ì›Œë“œ {missing_keywords}")
        
        print(f"âœ… Naver DataLab ë‹¤ì¤‘ ìˆ˜ì§‘ ì™„ë£Œ: {merged_df.shape}")
        snapshot_df(merged_df, "trends_naver")
        return merged_df
        
    except Exception as e:
        print(f"âŒ Naver DataLab ë‹¤ì¤‘ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        if STRICT_FETCH:
            raise
        return pd.DataFrame()

def datalab_timeseries(keyword: str, start: str, end: str, timeUnit: str = "date") -> pd.DataFrame:
    """
    Naver DataLab ì‹œê³„ì—´ ë°ì´í„° ìˆ˜ì§‘ (POST + Content-Type: application/json + JSON ë°”ë””)
    
    Args:
        keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
        start: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)
        end: ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)
        timeUnit: ì‹œê°„ ë‹¨ìœ„ (date, week, month)
        
    Returns:
        pd.DataFrame: ì‹œê³„ì—´ ë°ì´í„°
    """
    try:
        payload = {
            "startDate": start,
            "endDate": end,
            "timeUnit": timeUnit,
            "keywordGroups": [{"groupName": keyword, "keywords": [keyword]}],
            "device": "",
            "ages": [],
            "gender": ""
        }
        
        response = requests.post(
            "https://openapi.naver.com/v1/datalab/search",
            headers=_headers(json=True),
            json=payload
        )
        response.raise_for_status()
        
        data = response.json()
        results = data.get("results", [])
        
        if not results and STRICT_FETCH:
            raise ValueError(f"Naver DataLab ê²€ìƒ‰ ì‹¤íŒ¨: {keyword}")
        
        if results:
            df = pd.DataFrame(results[0]["data"])  # period, ratio
            _assert(df, ["period", "ratio"], "naver_datalab")
            return df
        else:
            return pd.DataFrame(columns=["period", "ratio"])
            
    except Exception as e:
        if STRICT_FETCH:
            raise
        print(f"Naver DataLab ê²€ìƒ‰ ì˜¤ë¥˜ ({keyword}): {e}")
        return pd.DataFrame(columns=["period", "ratio"])

class NaverCollector:
    """Naver ë°ì´í„° ìˆ˜ì§‘ í´ë˜ìŠ¤ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)"""
    
    def __init__(self, client_id: str, client_secret: str):
        self.client_id = client_id
        self.client_secret = client_secret
    
    def search_news(self, keywords: List[str], display: int = 100) -> List[Dict]:
        """ë‰´ìŠ¤ ê²€ìƒ‰ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
        news_data = []
        
        for keyword in keywords:
            try:
                df = search_news(keyword, display)
                
                for _, row in df.iterrows():
                    news_data.append({
                        'keyword': keyword,
                        'title': row.get('title', ''),
                        'description': row.get('desc', ''),
                        'link': row.get('url', ''),
                        'pub_date': row.get('published', ''),
                        'source': 'naver_news'
                    })
                    
            except Exception as e:
                print(f"Naver ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜ ({keyword}): {e}")
                continue
        
        return news_data
    
    def search_blog(self, keywords: List[str], display: int = 100) -> List[Dict]:
        """ë¸”ë¡œê·¸ ê²€ìƒ‰ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
        blog_data = []
        
        for keyword in keywords:
            try:
                df = search_blog(keyword, display)
                
                for _, row in df.iterrows():
                    blog_data.append({
                        'keyword': keyword,
                        'title': row.get('title', ''),
                        'description': row.get('desc', ''),
                        'link': row.get('url', ''),
                        'bloggername': row.get('bloggername', ''),
                        'postdate': row.get('published', ''),
                        'source': 'naver_blog'
                    })
                    
            except Exception as e:
                print(f"Naver ë¸”ë¡œê·¸ ê²€ìƒ‰ ì˜¤ë¥˜ ({keyword}): {e}")
                continue
        
        return blog_data
    
    def get_google_news_rss(self, keywords: List[str]) -> List[Dict]:
        """Google News RSS (ê¸°ì¡´ í˜¸í™˜ì„±)"""
        # ê¸°ì¡´ êµ¬í˜„ ìœ ì§€
        import feedparser
        
        news_data = []
        
        for keyword in keywords:
            try:
                rss_url = f"https://news.google.com/rss/search?q={keyword}&hl=ko&gl=KR&ceid=KR:ko"
                feed = feedparser.parse(rss_url)
                
                for entry in feed.entries:
                    news_data.append({
                        'keyword': keyword,
                        'title': entry.get('title', ''),
                        'description': entry.get('summary', ''),
                        'link': entry.get('link', ''),
                        'pub_date': entry.get('published', ''),
                        'source': 'google_news_rss'
                    })
                    
            except Exception as e:
                print(f"Google News RSS ìˆ˜ì§‘ ì˜¤ë¥˜ ({keyword}): {e}")
                continue
        
        return news_data
    
    def collect_all_news(self, keywords: List[str]) -> Dict[str, List[Dict]]:
        """ëª¨ë“  ë‰´ìŠ¤ ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ìˆ˜ì§‘ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
        result = {
            'naver_news': self.search_news(keywords),
            'naver_blog': self.search_blog(keywords),
            'google_news': self.get_google_news_rss(keywords)
        }
        
        return result