"""
Naver ë°ì´í„° ìˆ˜ì§‘ê¸° (ë‰´ìŠ¤, ë¸”ë¡œê·¸, DataLab)
"""
import os
import urllib.parse as up
import pandas as pd
import datetime as dt
import requests
from typing import List, Dict, Any

class NaverCollector:
    """Naver ë°ì´í„° ìˆ˜ì§‘ í´ë˜ìŠ¤"""
    
    def __init__(self, client_id: str, client_secret: str):
        self.client_id = client_id
        self.client_secret = client_secret
    
    def _headers(self, json=False):
        """API í˜¸ì¶œìš© í—¤ë” ìƒì„±"""
        h = {
            "X-Naver-Client-Id": self.client_id,
            "X-Naver-Client-Secret": self.client_secret
        }
        if json:
            h["Content-Type"] = "application/json"
        return h
    
    def search_news(self, query: str, display: int = 20) -> pd.DataFrame:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ (ê°œì„ ëœ ë²„ì „)"""
        print(f"ğŸ” NaverCollector.search_news í˜¸ì¶œë¨")
        print(f"ğŸ“ ì¿¼ë¦¬: {query}")
        print(f"ğŸ“ ì¿¼ë¦¬ íƒ€ì…: {type(query)}")
        print(f"ğŸ“ display: {display}")
        print(f"ğŸ“ API í‚¤ í™•ì¸: {self.client_id[:4] if self.client_id else 'None'}...")
        print(f"ğŸ“ API ì‹œí¬ë¦¿ í™•ì¸: {self.client_secret[:4] if self.client_secret else 'None'}...")
        
        if not self.client_id or not self.client_secret:
            print("âŒ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
            return pd.DataFrame()
        
        try:
            # í‚¤ì›Œë“œ ì „ì²˜ë¦¬ ë° í™•ì¥
            processed_queries = self._process_query(query)
            print(f"ğŸ“ ì²˜ë¦¬ëœ ì¿¼ë¦¬ë“¤: {processed_queries}")
            
            all_results = []
            
            # ê° ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ ìˆ˜í–‰
            for i, processed_query in enumerate(processed_queries):
                try:
                    print(f"ğŸ“¡ ê²€ìƒ‰ ì¿¼ë¦¬ {i+1}/{len(processed_queries)}: {processed_query}")
                    
                    # í‚¤ì›Œë“œë¥¼ ë” ì •í™•í•˜ê²Œ ì¸ì½”ë”©
                    q = up.quote(processed_query, safe='')
                    url = f"https://openapi.naver.com/v1/search/news.json?query={q}&display={min(display, 100)}&start=1&sort=date"
                    
                    print(f"ğŸ“ ì¸ì½”ë”©ëœ ì¿¼ë¦¬: {q}")
                    print(f"ğŸ“ ì „ì²´ URL: {url}")
                    
                    headers = {
                        "X-Naver-Client-Id": self.client_id,
                        "X-Naver-Client-Secret": self.client_secret
                    }
                    
                    response = requests.get(url, headers=headers)
                    print(f"ğŸ“¡ ì‘ë‹µ ìƒíƒœ: {response.status_code}")
                    
                    if response.status_code != 200:
                        print(f"âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {response.text}")
                        continue
                    
                    js = response.json()
                    items = js.get("items", [])
                    print(f"ğŸ“Š ì¿¼ë¦¬ '{processed_query}' ì‘ë‹µ ì•„ì´í…œ ìˆ˜: {len(items)}")
                    
                    if items:
                        # ê²°ê³¼ì— ì¿¼ë¦¬ ì •ë³´ ì¶”ê°€
                        for item in items:
                            item['search_query'] = processed_query
                            item['relevance_score'] = self._calculate_relevance(item, query)
                        
                        all_results.extend(items)
                        print(f"âœ… ì¿¼ë¦¬ '{processed_query}': {len(items)}ê°œ ê²°ê³¼ ì¶”ê°€")
                    
                except Exception as e:
                    print(f"âŒ ì¿¼ë¦¬ '{processed_query}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                    continue
            
            if not all_results:
                print("âš ï¸ ëª¨ë“  ì¿¼ë¦¬ì—ì„œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return pd.DataFrame()
            
            # DataFrame ìƒì„±
            df = pd.DataFrame([{
                "title": it.get("title", "").replace("<b>", "").replace("</b>", "").replace("&quot;", '"').replace("&amp;", "&"),
                "url": it.get("link", ""),
                "published": it.get("pubDate", ""),
                "desc": it.get("description", "").replace("<b>", "").replace("</b>", "").replace("&quot;", '"').replace("&amp;", "&"),
                "search_query": it.get("search_query", query),
                "relevance_score": it.get("relevance_score", 0)
            } for it in all_results])
            
            # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
            df = df.drop_duplicates(subset=['url'], keep='first')
            
            # ê´€ë ¨ì„± ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
            df = df.sort_values('relevance_score', ascending=False)
            
            # ê²°ê³¼ ê°œìˆ˜ ì œí•œ
            df = df.head(display)
            
            print(f"ğŸ“Š ìµœì¢… DataFrame ìƒì„± ì™„ë£Œ: {df.shape}")
            print(f"ğŸ“Š ê´€ë ¨ì„± ì ìˆ˜ ë¶„í¬: {df['relevance_score'].describe()}")
            
            return df
            
        except Exception as e:
            print(f"âŒ search_news ì—ëŸ¬: {e}")
            import traceback
            print(f"âŒ ìƒì„¸ ì—ëŸ¬: {traceback.format_exc()}")
            return pd.DataFrame()
    
    def _process_query(self, query: str) -> List[str]:
        """ì¿¼ë¦¬ ì „ì²˜ë¦¬ ë° í™•ì¥"""
        queries = [query]  # ê¸°ë³¸ ì¿¼ë¦¬
        
        # í‚¤ì›Œë“œë³„ íŠ¹ë³„ ì²˜ë¦¬
        if "ai" in query.lower() or "ì¸ê³µì§€ëŠ¥" in query:
            # AI ê´€ë ¨ ì¿¼ë¦¬ í™•ì¥
            if "ìƒì„±í˜•" in query or "generative" in query.lower():
                queries.extend([
                    f'"{query}"',  # ì •í™•í•œ êµ¬ë¬¸ ê²€ìƒ‰
                    f"{query} ê¸°ìˆ ",
                    f"{query} ë„êµ¬",
                    f"{query} ì„œë¹„ìŠ¤"
                ])
            else:
                queries.extend([
                    f'"{query}"',
                    f"{query} ê¸°ìˆ ",
                    f"{query} ë°œì „",
                    f"{query} ë™í–¥"
                ])
        else:
            # ì¼ë°˜ í‚¤ì›Œë“œ ì²˜ë¦¬
            queries.extend([
                f'"{query}"',  # ì •í™•í•œ êµ¬ë¬¸ ê²€ìƒ‰
                f"{query} ê´€ë ¨",
                f"{query} ë™í–¥"
            ])
        
        # ì¤‘ë³µ ì œê±° ë° ìˆœì„œ ìœ ì§€
        seen = set()
        unique_queries = []
        for q in queries:
            if q.lower() not in seen:
                seen.add(q.lower())
                unique_queries.append(q)
        
        return unique_queries[:5]  # ìµœëŒ€ 5ê°œ ì¿¼ë¦¬ë¡œ ì œí•œ
    
    def _calculate_relevance(self, item: Dict, original_query: str) -> int:
        """ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        try:
            title = str(item.get("title", "")).lower()
            desc = str(item.get("description", "")).lower()
            query = original_query.lower()
            
            score = 0
            
            # ì •í™•í•œ ë§¤ì¹­ (ê°€ì¥ ë†’ì€ ì ìˆ˜)
            if query in title:
                score += 20
            if query in desc:
                score += 10
            
            # êµ¬ë¬¸ ë§¤ì¹­ (ë”°ì˜´í‘œë¡œ ê°ì‹¸ì§„ ê²½ìš°)
            if f'"{query}"' in title or f'"{query}"' in desc:
                score += 15
            
            # ë‹¨ì–´ë³„ ë§¤ì¹­
            query_words = query.split()
            for word in query_words:
                if len(word) > 2:  # 2ê¸€ì ì´ìƒì¸ ë‹¨ì–´ë§Œ
                    if word in title:
                        score += 5
                    if word in desc:
                        score += 2
            
            # í‚¤ì›Œë“œë³„ ì¶”ê°€ ì ìˆ˜
            if "ai" in query.lower() or "ì¸ê³µì§€ëŠ¥" in query:
                ai_terms = ["ai", "ì¸ê³µì§€ëŠ¥", "ë¨¸ì‹ ëŸ¬ë‹", "ë”¥ëŸ¬ë‹", "chatgpt", "gpt"]
                for term in ai_terms:
                    if term in title:
                        score += 3
                    if term in desc:
                        score += 1
            
            return score
            
        except Exception as e:
            print(f"âŒ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0
    
    def search_blog(self, query: str, display: int = 20) -> pd.DataFrame:
        """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ (ê°œì„ ëœ ë²„ì „)"""
        print(f"ğŸ” NaverCollector.search_blog í˜¸ì¶œë¨")
        print(f"ğŸ“ ì¿¼ë¦¬: {query}")
        print(f"ğŸ“ ì¿¼ë¦¬ íƒ€ì…: {type(query)}")
        print(f"ğŸ“ display: {display}")
        print(f"ğŸ“ API í‚¤ í™•ì¸: {self.client_id[:4] if self.client_id else 'None'}...")
        print(f"ğŸ“ API ì‹œí¬ë¦¿ í™•ì¸: {self.client_secret[:4] if self.client_secret else 'None'}...")
        
        if not self.client_id or not self.client_secret:
            print("âŒ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
            return pd.DataFrame()
        
        try:
            # í‚¤ì›Œë“œ ì „ì²˜ë¦¬ ë° í™•ì¥
            processed_queries = self._process_query(query)
            print(f"ğŸ“ ì²˜ë¦¬ëœ ì¿¼ë¦¬ë“¤: {processed_queries}")
            
            all_results = []
            
            # ê° ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ ìˆ˜í–‰
            for i, processed_query in enumerate(processed_queries):
                try:
                    print(f"ğŸ“¡ ê²€ìƒ‰ ì¿¼ë¦¬ {i+1}/{len(processed_queries)}: {processed_query}")
                    
                    # í‚¤ì›Œë“œë¥¼ ë” ì •í™•í•˜ê²Œ ì¸ì½”ë”©
                    q = up.quote(processed_query, safe='')
                    url = f"https://openapi.naver.com/v1/search/blog.json?query={q}&display={min(display, 100)}&start=1&sort=date"
                    
                    print(f"ğŸ“ ì¸ì½”ë”©ëœ ì¿¼ë¦¬: {q}")
                    print(f"ğŸ“ ì „ì²´ URL: {url}")
                    
                    headers = {
                        "X-Naver-Client-Id": self.client_id,
                        "X-Naver-Client-Secret": self.client_secret
                    }
                    
                    response = requests.get(url, headers=headers)
                    print(f"ğŸ“¡ ì‘ë‹µ ìƒíƒœ: {response.status_code}")
                    
                    if response.status_code != 200:
                        print(f"âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {response.text}")
                        continue
                    
                    js = response.json()
                    items = js.get("items", [])
                    print(f"ğŸ“Š ì¿¼ë¦¬ '{processed_query}' ì‘ë‹µ ì•„ì´í…œ ìˆ˜: {len(items)}")
                    
                    if items:
                        # ê²°ê³¼ì— ì¿¼ë¦¬ ì •ë³´ ì¶”ê°€
                        for item in items:
                            item['search_query'] = processed_query
                            item['relevance_score'] = self._calculate_relevance(item, query)
                        
                        all_results.extend(items)
                        print(f"âœ… ì¿¼ë¦¬ '{processed_query}': {len(items)}ê°œ ê²°ê³¼ ì¶”ê°€")
                    
                except Exception as e:
                    print(f"âŒ ì¿¼ë¦¬ '{processed_query}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                    continue
            
            if not all_results:
                print("âš ï¸ ëª¨ë“  ì¿¼ë¦¬ì—ì„œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return pd.DataFrame()
            
            # DataFrame ìƒì„±
            df = pd.DataFrame([{
                "title": it.get("title", "").replace("<b>", "").replace("</b>", "").replace("&quot;", '"').replace("&amp;", "&"),
                "url": it.get("link", ""),
                "published": it.get("postdate", ""),
                "desc": it.get("description", "").replace("<b>", "").replace("</b>", "").replace("&quot;", '"').replace("&amp;", "&"),
                "bloggername": it.get("bloggername", ""),
                "search_query": it.get("search_query", query),
                "relevance_score": it.get("relevance_score", 0)
            } for it in all_results])
            
            # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
            df = df.drop_duplicates(subset=['url'], keep='first')
            
            # ê´€ë ¨ì„± ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
            df = df.sort_values('relevance_score', ascending=False)
            
            # ê²°ê³¼ ê°œìˆ˜ ì œí•œ
            df = df.head(display)
            
            print(f"ğŸ“Š ìµœì¢… DataFrame ìƒì„± ì™„ë£Œ: {df.shape}")
            print(f"ğŸ“Š ê´€ë ¨ì„± ì ìˆ˜ ë¶„í¬: {df['relevance_score'].describe()}")
            
            return df
            
        except Exception as e:
            print(f"âŒ search_blog ì—ëŸ¬: {e}")
            import traceback
            print(f"âŒ ìƒì„¸ ì—ëŸ¬: {traceback.format_exc()}")
            return pd.DataFrame()

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ
NAVER_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_SECRET = os.getenv("NAVER_CLIENT_SECRET")

# STRICT_FETCH ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¡œë“œ, ê¸°ë³¸ê°’ False)
STRICT_FETCH = os.getenv("STRICT_FETCH", "False").lower() == "true"

def _headers(json=False):
    """API í˜¸ì¶œìš© í—¤ë” ìƒì„±"""
    h = {
        "X-Naver-Client-Id": NAVER_ID,
        "X-Naver-Client-Secret": NAVER_SECRET
    }
    if json:
        h["Content-Type"] = "application/json"
    return h

def _assert(df, cols, name="dataset", min_rows=3):
    """ë°ì´í„°í”„ë ˆì„ ê²€ì¦ ìœ í‹¸ë¦¬í‹°"""
    if df is None or len(df) < min_rows:
        raise ValueError(f"{name}: too few rows (expected >= {min_rows}, got {len(df) if df is not None else 0})")
    missing = [c for c in cols if c not in df.columns]
    if missing:
        raise ValueError(f"{name}: missing columns {missing}")

def search_news(query: str, display: int = 20) -> pd.DataFrame:
    """
    Naver ë‰´ìŠ¤ ê²€ìƒ‰ (GET + í—¤ë” 2ê°œ) + ë””ë²„ê¹…
    
    Args:
        query: ê²€ìƒ‰ í‚¤ì›Œë“œ
        display: ê²€ìƒ‰ ê²°ê³¼ ìˆ˜
        
    Returns:
        pd.DataFrame: ë‰´ìŠ¤ ë°ì´í„°
    """
    print(f"ğŸ” Naver ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘: '{query}', {display}ê°œ ê²°ê³¼")
    print(f"ğŸ“ ì…ë ¥ íŒŒë¼ë¯¸í„° - query: {query}, display: {display}")
    print(f"ğŸ“ query íƒ€ì…: {type(query)}")
    print(f"ğŸ“ API í‚¤ í™•ì¸: {NAVER_ID[:4] if NAVER_ID else 'None'}...")
    
    try:
        q = up.quote(query)
        url = f"https://openapi.naver.com/v1/search/news.json?query={q}&display={display}&start=1&sort=date"
        
        print(f"ğŸ“¡ API URL: {url}")
        print(f"ğŸ“¡ í—¤ë”: {_headers()}")
        
        response = requests.get(url, headers=_headers())
        print(f"ğŸ“¡ ì‘ë‹µ ìƒíƒœ ì½”ë“œ: {response.status_code}")
        
        if response.status_code != 200:
            print(f"âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
            print(f"âŒ ì‘ë‹µ ë‚´ìš©: {response.text}")
            response.raise_for_status()
        
        js = response.json()
        print(f"ğŸ“Š JSON ì‘ë‹µ: {js}")
        
        items = js.get("items", [])
        print(f"ğŸ“Š API ì‘ë‹µ: {len(items)}ê°œ ì•„ì´í…œ")
        
        if not items and STRICT_FETCH:
            raise ValueError(f"Naver ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {query}")
        
        # ë°ì´í„° í’ˆì§ˆ ê²€ì¦
        valid_items = []
        for i, it in enumerate(items):
            title = it.get("title", "")
            desc = it.get("description", "")
            
            print(f"ğŸ“ ì•„ì´í…œ {i}: title='{title}', desc='{desc[:50]}...'")
            
            # HTML íƒœê·¸ ì œê±° í™•ì¸
            clean_title = title.replace("<b>", "").replace("</b>", "").replace("&quot;", '"').replace("&amp;", "&")
            clean_desc = desc.replace("<b>", "").replace("</b>", "").replace("&quot;", '"').replace("&amp;", "&")
            
            if i < 3:  # ì²˜ìŒ 3ê°œ ì•„ì´í…œë§Œ ë””ë²„ê¹… ì¶œë ¥
                print(f"  ğŸ“° ì•„ì´í…œ {i+1}:")
                print(f"    ì œëª©: {clean_title[:50]}...")
                print(f"    ì„¤ëª…: {clean_desc[:50]}...")
            
            valid_items.append({
                "title": clean_title,
                "url": it.get("link", ""),
                "published": it.get("pubDate", ""),
                "desc": clean_desc
            })
        
        df = pd.DataFrame(valid_items)
        
        print(f"âœ… Naver ë‰´ìŠ¤ ë°ì´í„° ìƒì„±: {df.shape}")
        _assert(df, ["title", "url", "published"], "naver_news")
        return df
        
    except Exception as e:
        if STRICT_FETCH:
            raise
        print(f"âŒ Naver ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜ ({query}): {e}")
        return pd.DataFrame(columns=["title", "url", "published", "desc"])

def search_blog(query: str, display: int = 20) -> pd.DataFrame:
    """
    Naver ë¸”ë¡œê·¸ ê²€ìƒ‰ (GET + í—¤ë” 2ê°œ)
    
    Args:
        query: ê²€ìƒ‰ í‚¤ì›Œë“œ
        display: ê²€ìƒ‰ ê²°ê³¼ ìˆ˜
        
    Returns:
        pd.DataFrame: ë¸”ë¡œê·¸ ë°ì´í„°
    """
    print(f"ğŸ” Naver ë¸”ë¡œê·¸ ê²€ìƒ‰ ì‹œì‘: '{query}', {display}ê°œ ê²°ê³¼")
    print(f"ğŸ“ ì…ë ¥ íŒŒë¼ë¯¸í„° - query: {query}, display: {display}")
    print(f"ğŸ“ query íƒ€ì…: {type(query)}")
    print(f"ğŸ“ API í‚¤ í™•ì¸: {NAVER_ID[:4] if NAVER_ID else 'None'}...")
    
    try:
        q = up.quote(query)
        url = f"https://openapi.naver.com/v1/search/blog.json?query={q}&display={display}&start=1&sort=date"
        
        print(f"ğŸ“¡ API URL: {url}")
        print(f"ğŸ“¡ í—¤ë”: {_headers()}")
        
        response = requests.get(url, headers=_headers())
        print(f"ğŸ“¡ ì‘ë‹µ ìƒíƒœ ì½”ë“œ: {response.status_code}")
        
        if response.status_code != 200:
            print(f"âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
            print(f"âŒ ì‘ë‹µ ë‚´ìš©: {response.text}")
            response.raise_for_status()
        
        js = response.json()
        print(f"ğŸ“Š JSON ì‘ë‹µ: {js}")
        
        items = js.get("items", [])
        print(f"ğŸ“Š API ì‘ë‹µ: {len(items)}ê°œ ì•„ì´í…œ")
        
        if not items and STRICT_FETCH:
            raise ValueError(f"Naver ë¸”ë¡œê·¸ ê²€ìƒ‰ ì‹¤íŒ¨: {query}")
        
        print(f"ğŸ“Š DataFrame ìƒì„± ì‹œì‘...")
        df = pd.DataFrame([{
            "title": it.get("title", ""),
            "url": it.get("link", ""),
            "published": it.get("postdate", ""),
            "desc": it.get("description", ""),
            "bloggername": it.get("bloggername", "")
        } for it in items])
        
        print(f"ğŸ“Š DataFrame ìƒì„± ì™„ë£Œ: {df.shape}")
        print(f"ğŸ“Š DataFrame ì»¬ëŸ¼: {df.columns.tolist()}")
        print(f"ğŸ“Š DataFrame ìƒ˜í”Œ: {df.head()}")
        
        _assert(df, ["title", "url", "published"], "naver_blog")
        print(f"âœ… ê²€ì¦ í†µê³¼")
        return df
        
    except Exception as e:
        print(f"âŒ search_blog ì—ëŸ¬: {e}")
        print(f"âŒ ì—ëŸ¬ íƒ€ì…: {type(e)}")
        import traceback
        print(f"âŒ ìƒì„¸ ì—ëŸ¬: {traceback.format_exc()}")
        
        if STRICT_FETCH:
            raise
        print(f"Naver ë¸”ë¡œê·¸ ê²€ìƒ‰ ì˜¤ë¥˜ ({query}): {e}")
        return pd.DataFrame(columns=["title", "url", "published", "desc", "bloggername"])

def datalab_timeseries_multi(keywords: list, start: str, end: str, timeUnit: str = "date") -> pd.DataFrame:
    """
    Naver DataLab ë‹¤ì¤‘ í‚¤ì›Œë“œ ì‹œê³„ì—´ ë°ì´í„° ìˆ˜ì§‘ (POST + Content-Type: application/json + JSON ë°”ë””)
    
    Args:
        keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        start: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)
        end: ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)
        timeUnit: ì‹œê°„ ë‹¨ìœ„ (date, week, month)
        
    Returns:
        pd.DataFrame: ì‹œê³„ì—´ ë°ì´í„° (period + ê° í‚¤ì›Œë“œ ì»¬ëŸ¼)
        
    Raises:
        ValueError: ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ ë˜ëŠ” ë¹ˆ ê²°ê³¼
    """
    from src.common.trace import snapshot_df, log_shape
    
    try:
        print(f"Naver DataLab ë‹¤ì¤‘ ìˆ˜ì§‘ ì‹œì‘: {keywords}, ê¸°ê°„: {start} ~ {end}")
        
        # í‚¤ì›Œë“œ ê·¸ë£¹ ìƒì„±
        keyword_groups = []
        for keyword in keywords:
            keyword_groups.append({
                "groupName": keyword,
                "keywords": [keyword]
            })
        
        payload = {
            "startDate": start,
            "endDate": end,
            "timeUnit": timeUnit,
            "keywordGroups": keyword_groups,
            "device": "",
            "ages": [],
            "gender": ""
        }
        
        response = requests.post(
            "https://openapi.naver.com/v1/datalab/search",
            headers=_headers(json=True),
            json=payload
        )
        response.raise_for_status()
        
        data = response.json()
        results = data.get("results", [])
        
        if not results:
            raise ValueError(f"Naver DataLab: ë¹ˆ ê²°ê³¼ {keywords}")
        
        # ê²°ê³¼ë¥¼ wide í˜•íƒœë¡œ ë¨¸ì§€
        merged_df = None
        for result in results:
            keyword_name = result["title"]
            keyword_data = pd.DataFrame(result["data"])
            
            if keyword_data.empty:
                print(f"âš ï¸ {keyword_name}: ë¹ˆ ë°ì´í„°")
                continue
                
            keyword_data = keyword_data.rename(columns={"ratio": keyword_name})
            keyword_data = keyword_data.set_index("period")
            
            if merged_df is None:
                merged_df = keyword_data
            else:
                merged_df = merged_df.join(keyword_data, how='outer')
        
        if merged_df is None or merged_df.empty:
            raise ValueError(f"Naver DataLab: ìµœì¢… ë¹ˆ ê²°ê³¼ {keywords}")
        
        # ì¸ë±ìŠ¤ë¥¼ ë‹¤ì‹œ ì»¬ëŸ¼ìœ¼ë¡œ
        merged_df = merged_df.reset_index()
        
        # í‚¤ì›Œë“œ ì»¬ëŸ¼ í™•ì¸
        missing_keywords = [kw for kw in keywords if kw not in merged_df.columns]
        if missing_keywords:
            raise ValueError(f"Naver DataLab: ëˆ„ë½ëœ í‚¤ì›Œë“œ {missing_keywords}")
        
        print(f"âœ… Naver DataLab ë‹¤ì¤‘ ìˆ˜ì§‘ ì™„ë£Œ: {merged_df.shape}")
        snapshot_df(merged_df, "trends_naver")
        return merged_df
        
    except Exception as e:
        print(f"âŒ Naver DataLab ë‹¤ì¤‘ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        if STRICT_FETCH:
            raise
        return pd.DataFrame()

def datalab_timeseries(keyword: str, start: str, end: str, timeUnit: str = "date") -> pd.DataFrame:
    """
    Naver DataLab ì‹œê³„ì—´ ë°ì´í„° ìˆ˜ì§‘ (POST + Content-Type: application/json + JSON ë°”ë””)
    
    Args:
        keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
        start: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)
        end: ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)
        timeUnit: ì‹œê°„ ë‹¨ìœ„ (date, week, month)
        
    Returns:
        pd.DataFrame: ì‹œê³„ì—´ ë°ì´í„°
    """
    try:
        payload = {
            "startDate": start,
            "endDate": end,
            "timeUnit": timeUnit,
            "keywordGroups": [{"groupName": keyword, "keywords": [keyword]}],
            "device": "",
            "ages": [],
            "gender": ""
        }
        
        response = requests.post(
            "https://openapi.naver.com/v1/datalab/search",
            headers=_headers(json=True),
            json=payload
        )
        response.raise_for_status()
        
        data = response.json()
        results = data.get("results", [])
        
        if not results and STRICT_FETCH:
            raise ValueError(f"Naver DataLab ê²€ìƒ‰ ì‹¤íŒ¨: {keyword}")
        
        if results:
            df = pd.DataFrame(results[0]["data"])  # period, ratio
            _assert(df, ["period", "ratio"], "naver_datalab")
            return df
        else:
            return pd.DataFrame(columns=["period", "ratio"])
            
    except Exception as e:
        if STRICT_FETCH:
            raise
        print(f"Naver DataLab ê²€ìƒ‰ ì˜¤ë¥˜ ({keyword}): {e}")
        return pd.DataFrame(columns=["period", "ratio"])

class NaverCollector:
    """Naver ë°ì´í„° ìˆ˜ì§‘ í´ë˜ìŠ¤ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)"""
    
    def __init__(self, client_id: str, client_secret: str):
        self.client_id = client_id
        self.client_secret = client_secret
    
    def search_news(self, keywords: List[str], display: int = 100) -> List[Dict]:
        """ë‰´ìŠ¤ ê²€ìƒ‰ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
        news_data = []
        
        for keyword in keywords:
            try:
                df = search_news(keyword, display)
                
                for _, row in df.iterrows():
                    news_data.append({
                        'keyword': keyword,
                        'title': row.get('title', ''),
                        'description': row.get('desc', ''),
                        'link': row.get('url', ''),
                        'pub_date': row.get('published', ''),
                        'source': 'naver_news'
                    })
                    
            except Exception as e:
                print(f"Naver ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜ ({keyword}): {e}")
                continue
        
        return news_data
    
    def search_blog(self, keywords: List[str], display: int = 100) -> List[Dict]:
        """ë¸”ë¡œê·¸ ê²€ìƒ‰ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
        blog_data = []
        
        for keyword in keywords:
            try:
                df = search_blog(keyword, display)
                
                for _, row in df.iterrows():
                    blog_data.append({
                        'keyword': keyword,
                        'title': row.get('title', ''),
                        'description': row.get('desc', ''),
                        'link': row.get('url', ''),
                        'bloggername': row.get('bloggername', ''),
                        'postdate': row.get('published', ''),
                        'source': 'naver_blog'
                    })
                    
            except Exception as e:
                print(f"Naver ë¸”ë¡œê·¸ ê²€ìƒ‰ ì˜¤ë¥˜ ({keyword}): {e}")
                continue
        
        return blog_data
    
    def get_google_news_rss(self, keywords: List[str]) -> List[Dict]:
        """Google News RSS (ê¸°ì¡´ í˜¸í™˜ì„±)"""
        # ê¸°ì¡´ êµ¬í˜„ ìœ ì§€
        import feedparser
        
        news_data = []
        
        for keyword in keywords:
            try:
                rss_url = f"https://news.google.com/rss/search?q={keyword}&hl=ko&gl=KR&ceid=KR:ko"
                feed = feedparser.parse(rss_url)
                
                for entry in feed.entries:
                    news_data.append({
                        'keyword': keyword,
                        'title': entry.get('title', ''),
                        'description': entry.get('summary', ''),
                        'link': entry.get('link', ''),
                        'pub_date': entry.get('published', ''),
                        'source': 'google_news_rss'
                    })
                    
            except Exception as e:
                print(f"Google News RSS ìˆ˜ì§‘ ì˜¤ë¥˜ ({keyword}): {e}")
                continue
        
        return news_data
    
    def collect_all_news(self, keywords: List[str]) -> Dict[str, List[Dict]]:
        """ëª¨ë“  ë‰´ìŠ¤ ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ìˆ˜ì§‘ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
        result = {
            'naver_news': self.search_news(keywords),
            'naver_blog': self.search_blog(keywords),
            'google_news': self.get_google_news_rss(keywords)
        }
        
        return result