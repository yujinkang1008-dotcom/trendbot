"""
arXiv í•™ìˆ  ë…¼ë¬¸ ë°ì´í„° ìˆ˜ì§‘ê¸°
"""
import requests
import feedparser
from typing import List, Dict, Any
from datetime import datetime, timedelta
import time
import pandas as pd
from src.nlp.clean import normalize_for_topics

class ArxivCollector:
    """arXiv í•™ìˆ  ë…¼ë¬¸ ë°ì´í„° ìˆ˜ì§‘ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.base_url = "http://export.arxiv.org/api/query"
        
    def collect_papers(self, keywords: List[str], max_results: int = 500, start_date: str = None, end_date: str = None) -> pd.DataFrame:
        """
        arXivì—ì„œ ë…¼ë¬¸ ê²€ìƒ‰ ë° í…ìŠ¤íŠ¸ ì •ì œ + ë””ë²„ê¹…
        
        Args:
            keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            max_results: ìµœëŒ€ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜
            start_date: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD, ì„ íƒì‚¬í•­)
            end_date: ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD, ì„ íƒì‚¬í•­)
            
        Returns:
            pd.DataFrame: ë…¼ë¬¸ ë°ì´í„° (title, url, published, summary, text_clean)
            
        Raises:
            ValueError: ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ ë˜ëŠ” ë¹ˆ ê²°ê³¼
        """
        print(f"ğŸ” arXiv ë…¼ë¬¸ ìˆ˜ì§‘ ì‹œì‘: {keywords}, ìµœëŒ€ {max_results}ê°œ")
        
        papers_data = []
        
        for keyword in keywords:
            try:
                print(f"ğŸ“š í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì¤‘...")
                
                # arXiv API íŒŒë¼ë¯¸í„° (í•œêµ­ì–´ í‚¤ì›Œë“œ ì²˜ë¦¬)
                # í•œêµ­ì–´ í‚¤ì›Œë“œì¸ ê²½ìš° ì˜ì–´ ë²ˆì—­ ì‹œë„
                search_query = keyword
                if any('\uac00' <= char <= '\ud7a3' for char in keyword):  # í•œê¸€ í¬í•¨ ì—¬ë¶€
                    # í•œêµ­ì–´ í‚¤ì›Œë“œë¥¼ ì˜ì–´ë¡œ ë§¤í•‘
                    korean_to_english = {
                        'ì¸ê³µì§€ëŠ¥': 'artificial intelligence',
                        'ë¨¸ì‹ ëŸ¬ë‹': 'machine learning',
                        'ë”¥ëŸ¬ë‹': 'deep learning',
                        'ìì—°ì–´ì²˜ë¦¬': 'natural language processing',
                        'ì»´í“¨í„°ë¹„ì „': 'computer vision',
                        'ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤': 'data science',
                        'ë¹…ë°ì´í„°': 'big data',
                        'ë¸”ë¡ì²´ì¸': 'blockchain',
                        'ì‚¬ì´ë²„ë³´ì•ˆ': 'cybersecurity',
                        'í´ë¼ìš°ë“œ': 'cloud computing'
                    }
                    search_query = korean_to_english.get(keyword, 'artificial intelligence')
                    print(f"  ğŸ”„ í•œêµ­ì–´ í‚¤ì›Œë“œ '{keyword}' -> ì˜ì–´ '{search_query}'ë¡œ ë³€í™˜")
                
                params = {
                    'search_query': f'all:{search_query}',
                    'start': 0,
                    'max_results': max_results,
                    'sortBy': 'submittedDate',
                    'sortOrder': 'descending'
                }
                
                print(f"ğŸ“¡ API íŒŒë¼ë¯¸í„°: {params}")
                
                # API í˜¸ì¶œ
                response = requests.get(self.base_url, params=params)
                response.raise_for_status()
                
                # XML íŒŒì‹±
                feed = feedparser.parse(response.content)
                
                print(f"ğŸ“Š íŒŒì‹±ëœ ì—”íŠ¸ë¦¬ ìˆ˜: {len(feed.entries)}")
                
                if not feed.entries:
                    print(f"âš ï¸ '{keyword}'ì— ëŒ€í•œ ê²°ê³¼ ì—†ìŒ")
                    continue
                
                keyword_papers = 0
                for i, entry in enumerate(feed.entries):
                    title = entry.get('title', '').strip()
                    summary = entry.get('summary', '').strip()
                    published = entry.get('published', '')
                    url = entry.get('link', '')
                    
                    # ê¸°ê°„ í•„í„°ë§ (published ê¸°ì¤€)
                    if start_date or end_date:
                        try:
                            pub_date = datetime.strptime(published[:10], '%Y-%m-%d')
                            if start_date and pub_date < datetime.strptime(start_date, '%Y-%m-%d'):
                                continue
                            if end_date and pub_date > datetime.strptime(end_date, '%Y-%m-%d'):
                                continue
                        except:
                            pass  # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê±´ë„ˆë›°ê¸°
                    
                    # title + summary ê²°í•©
                    text_raw = f"{title} {summary}".strip()
                    
                    if not text_raw:
                        continue
                    
                    # í…ìŠ¤íŠ¸ ì •ì œ
                    text_clean = normalize_for_topics(text_raw)
                    
                    # ë””ë²„ê¹…: ì²˜ìŒ 3ê°œ ë…¼ë¬¸ë§Œ ì¶œë ¥
                    if keyword_papers < 3:
                        print(f"  ğŸ“„ ë…¼ë¬¸ {keyword_papers+1}:")
                        print(f"    ì œëª©: {title[:50]}...")
                        print(f"    ì›ë³¸ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text_raw)}")
                        print(f"    ì •ì œëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text_clean)}")
                        print(f"    ì •ì œëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ: {text_clean[:100]}...")
                    
                    papers_data.append({
                        'title': title,
                        'url': url,
                        'published': published,
                        'summary': summary,
                        'text_clean': text_clean,
                        'keyword': keyword
                    })
                    
                    keyword_papers += 1
                
                print(f"âœ… '{keyword}': {keyword_papers}ê°œ ë…¼ë¬¸ ìˆ˜ì§‘")
                
                # API í˜¸ì¶œ ì œí•œ ê³ ë ¤
                time.sleep(1)
                
            except Exception as e:
                print(f"âŒ arXiv ê²€ìƒ‰ ì˜¤ë¥˜ ({keyword}): {e}")
                continue
        
        if not papers_data:
            print("âŒ arXiv: ìˆ˜ì§‘ëœ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤")
            raise ValueError("arXiv: ìˆ˜ì§‘ëœ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤")
        
        # DataFrame ìƒì„±
        df = pd.DataFrame(papers_data)
        
        print(f"ğŸ“Š DataFrame ìƒì„±: {df.shape}")
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_cols = ['title', 'url', 'published', 'summary', 'text_clean']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            print(f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_cols}")
            raise ValueError(f"arXiv: í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½ {missing_cols}")
        
        # text_clean í’ˆì§ˆ ê²€ì¦
        empty_clean = df['text_clean'].isna() | (df['text_clean'] == '')
        clean_ratio = (len(df) - empty_clean.sum()) / len(df) if len(df) > 0 else 0
        
        print(f"ğŸ“ˆ text_clean í’ˆì§ˆ: {clean_ratio:.2%} ({len(df) - empty_clean.sum()}/{len(df)})")
        
        if clean_ratio < 0.5:
            print("âš ï¸ text_clean í’ˆì§ˆì´ ë‚®ìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ ì •ì œ ë¡œì§ì„ í™•ì¸í•˜ì„¸ìš”.")
        
        print(f"âœ… arXiv ìˆ˜ì§‘ ì™„ë£Œ: {len(df)}ê°œ ë…¼ë¬¸")
        return df

    # ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”ì„œë“œ
    def search_papers(self, keywords: List[str], max_results: int = 100) -> List[Dict]:
        """ê¸°ì¡´ í˜¸í™˜ì„±ìš© ë©”ì„œë“œ"""
        try:
            df = self.collect_papers(keywords, max_results)
            return df.to_dict('records')
        except Exception as e:
            print(f"arXiv ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []