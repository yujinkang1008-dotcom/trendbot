"""
í† í”½ ì¶”ì¶œ ëª¨ë“ˆ (í˜•íƒœì†Œ ë¶„ì„ê¸° í†µí•©)
"""
from typing import List, Dict, Any
import re
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import numpy as np
import pandas as pd
from src.nlp.clean import STOPWORDS, GARBAGE_TOKENS
from src.nlp.morphological_analyzer import MorphologicalAnalyzer

class TopicExtractor:
    """í† í”½ ì¶”ì¶œ í´ë˜ìŠ¤"""
    
    def __init__(self, use_morphology: bool = True):
        """
        í† í”½ ì¶”ì¶œê¸° ì´ˆê¸°í™”
        
        Args:
            use_morphology: í˜•íƒœì†Œ ë¶„ì„ê¸° ì‚¬ìš© ì—¬ë¶€
        """
        self.use_morphology = use_morphology
        
        # í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” (KOMORAN ìš°ì„  ì‚¬ìš©)
        if self.use_morphology:
            try:
                # KOMORANì„ ìš°ì„  ì‹œë„
                self.morph_analyzer = MorphologicalAnalyzer("komoran")
                print("âœ… KOMORAN í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ KOMORAN í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                try:
                    # Oktë¡œ ëŒ€ì²´ ì‹œë„
                    self.morph_analyzer = MorphologicalAnalyzer("okt")
                    print("âœ… Okt í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
                except Exception as e2:
                    print(f"âš ï¸ Okt í˜•íƒœì†Œ ë¶„ì„ê¸°ë„ ì‹¤íŒ¨: {e2}")
                    print("ğŸ”„ ê¸°ë³¸ í† í°í™” ë°©ì‹ìœ¼ë¡œ ì „í™˜")
                    self.use_morphology = False
                    self.morph_analyzer = None
        else:
            self.morph_analyzer = None
        
        # ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (í•œêµ­ì–´) - ê¸°ë³¸ ë¶ˆìš©ì–´
        self.stop_words = {
            'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ', 'ì˜', 'ì™€', 'ê³¼', 'ë„', 'ëŠ”', 'ì€',
            'í•˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ë˜ë‹¤', 'ì´ë‹¤', 'ì•„ë‹ˆë‹¤', 'ê·¸', 'ì´', 'ì €', 'ê·¸ê²ƒ', 'ì´ê²ƒ', 'ì €ê²ƒ',
            'ê·¸ëŸ°', 'ì´ëŸ°', 'ì €ëŸ°', 'ê·¸ë ‡ê²Œ', 'ì´ë ‡ê²Œ', 'ì €ë ‡ê²Œ', 'ê·¸ë•Œ', 'ì´ë•Œ', 'ì €ë•Œ',
            'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë˜í•œ', 'ë˜', 'ê·¸ë˜ì„œ', 'ë”°ë¼ì„œ', 'ê·¸ëŸ¬ë¯€ë¡œ',
            'ìœ„í•´', 'ìœ„í•œ', 'ëŒ€í•´', 'ëŒ€í•œ', 'ê´€í•´', 'ê´€í•œ', 'í†µí•´', 'í†µí•œ',
            'ë•Œë¬¸', 'ë•Œë¬¸ì—', 'ë•ë¶„', 'ë•ë¶„ì—', 'ë¹„í•´', 'ë¹„í•´', 'ëŒ€ë¹„', 'ëŒ€ë¹„í•´',
            'ê´€ë ¨', 'ê´€ë ¨ëœ', 'ê´€ë ¨í•´', 'ê´€ë ¨í•´ì„œ', 'ê´€ë ¨ë˜ë‹¤', 'ê´€ë ¨ëœë‹¤',
            'ê²½ìš°', 'ê²½ìš°ì—', 'ê²½ìš°ì—ëŠ”', 'ê²½ìš°', 'ê²½ìš°ì—', 'ê²½ìš°ì—ëŠ”',
            'ìˆ˜', 'ìˆ˜ë„', 'ìˆ˜ëŠ”', 'ìˆ˜ë§Œ', 'ìˆ˜ë§Œí¼', 'ìˆ˜ë§Œí¼ì´ë‚˜',
            'ê²ƒ', 'ê²ƒì´', 'ê²ƒì„', 'ê²ƒì—', 'ê²ƒìœ¼ë¡œ', 'ê²ƒìœ¼ë¡œì„œ', 'ê²ƒìœ¼ë¡œì¨',
            'ë“±', 'ë“±ì˜', 'ë“±ì´', 'ë“±ì„', 'ë“±ì—', 'ë“±ìœ¼ë¡œ', 'ë“±ìœ¼ë¡œì„œ', 'ë“±ìœ¼ë¡œì¨',
            'ë°', 'ê·¸ë¦¬ê³ ', 'ë˜í•œ', 'ë˜', 'ê·¸ë˜ì„œ', 'ë”°ë¼ì„œ', 'ê·¸ëŸ¬ë¯€ë¡œ',
            'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ê·¸ëŸ°ë°', 'ê·¸ëŸ°', 'ì´ëŸ°', 'ì €ëŸ°',
            'ê·¸ë ‡ê²Œ', 'ì´ë ‡ê²Œ', 'ì €ë ‡ê²Œ', 'ê·¸ë•Œ', 'ì´ë•Œ', 'ì €ë•Œ',
            'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë˜í•œ', 'ë˜', 'ê·¸ë˜ì„œ', 'ë”°ë¼ì„œ', 'ê·¸ëŸ¬ë¯€ë¡œ'
        }
    
    def preprocess_text(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        
        Args:
            text: ì „ì²˜ë¦¬í•  í…ìŠ¤íŠ¸
            
        Returns:
            str: ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸
        """
        if not text:
            return ""
        
        # íŠ¹ìˆ˜ë¬¸ì ì œê±°
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # ìˆ«ì ì œê±°
        text = re.sub(r'\d+', '', text)
        
        # ê³µë°± ì •ë¦¬
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def extract_keywords(self, texts: List[str], max_keywords: int = 50) -> List[Dict]:
        """
        í‚¤ì›Œë“œ ì¶”ì¶œ (í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜)
        
        Args:
            texts: ë¶„ì„í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            max_keywords: ìµœëŒ€ í‚¤ì›Œë“œ ìˆ˜
            
        Returns:
            List[Dict]: í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ [{'keyword': 'AI', 'count': 50, 'pos': 'Noun'}, ...]
        """
        if not texts:
            return []
        
        # í˜•íƒœì†Œ ë¶„ì„ê¸° ì‚¬ìš©
        if self.use_morphology and self.morph_analyzer:
            return self._extract_keywords_with_morphology(texts, max_keywords)
        else:
            return self._extract_keywords_basic(texts, max_keywords)
    
    def _extract_keywords_with_morphology(self, texts: List[str], max_keywords: int) -> List[Dict]:
        """
        í˜•íƒœì†Œ ë¶„ì„ì„ ì´ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
        
        Args:
            texts: ë¶„ì„í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            max_keywords: ìµœëŒ€ í‚¤ì›Œë“œ ìˆ˜
            
        Returns:
            List[Dict]: í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        print(f"ğŸ” í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ: {len(texts)}ê°œ í…ìŠ¤íŠ¸")
        
        all_keywords = []
        for i, text in enumerate(texts):
            if text:
                print(f"  ğŸ“ í…ìŠ¤íŠ¸ {i+1}: {text[:50]}...")
                keywords = self.morph_analyzer.extract_keywords(text, max_keywords=100)
                print(f"  ğŸ”‘ ì¶”ì¶œëœ í‚¤ì›Œë“œ: {len(keywords)}ê°œ")
                if keywords:
                    print(f"  ğŸ“‹ í‚¤ì›Œë“œ ìƒ˜í”Œ: {[kw['keyword'] for kw in keywords[:5]]}")
                all_keywords.extend(keywords)
        
        # ë¹ˆë„ ê³„ì‚°
        keyword_counts = Counter([kw['keyword'] for kw in all_keywords])
        
        # ìƒìœ„ í‚¤ì›Œë“œ ì„ íƒ
        top_keywords = keyword_counts.most_common(max_keywords)
        
        # ê²°ê³¼ ì •ë¦¬ (HTML/ì›¹ ê´€ë ¨ ë¶ˆìš©ì–´ ì¶”ê°€ í•„í„°ë§ - ê°•í™”)
        html_stopwords = {
            'nbsp', 'font', 'href', 'blank', 'target', 'src', 'img', 'div', 'span', 'class', 'id',
            'script', 'css', 'js', 'jquery', 'ajax', 'json', 'xml', 'html', 'htm', 'php', 'asp',
            'http', 'https', 'www', 'com', 'net', 'org', 'co', 'kr', 'link', 'url',
            'rss', 'feed', 'atom', 'syndication', 'channel', 'item', 'description', 'pubdate',
            'guid', 'category', 'enclosure', 'articles', 'target', 'oc', 'ios', 'android',
            'windows', 'mac', 'linux', 'google', 'news', 'naver', 'daum', 'yahoo', 'bing',
            'search', 'youtube', 'facebook', 'twitter', 'instagram', 'linkedin', 'github',
            # HTML ì†ì„±ê°’ë“¤
            'color', 'red', 'blue', 'green', 'yellow', 'black', 'white', 'gray', 'grey',
            'size', 'width', 'height', 'border', 'margin', 'padding', 'background',
            'display', 'position', 'float', 'clear', 'overflow', 'visible', 'hidden',
            'block', 'inline', 'table', 'cell', 'row', 'column', 'header', 'footer',
            'left', 'right', 'center', 'top', 'bottom', 'middle', 'start', 'end',
            'bold', 'italic', 'underline', 'strike', 'normal', 'small', 'large', 'big',
            'tiny', 'huge', 'massive', 'mini', 'micro', 'macro', 'full', 'empty',
            'half', 'quarter', 'double', 'triple', 'single', 'multiple', 'first', 'last',
            'next', 'prev', 'previous', 'back', 'forward', 'up', 'down', 'over', 'under',
            'above', 'below', 'before', 'after', 'during', 'while', 'since', 'until',
            'within', 'without', 'inside', 'outside', 'beside', 'near', 'far', 'close',
            'distant', 'local', 'global', 'national', 'international', 'regional',
            'urban', 'rural', 'domestic', 'foreign', 'internal', 'external', 'public',
            'private', 'personal', 'individual', 'collective', 'group', 'team',
            'organization', 'company', 'business', 'industry', 'sector', 'field',
            'area', 'region', 'country', 'nation', 'state', 'city', 'town', 'village',
            'community', 'society', 'culture', 'tradition', 'custom', 'habit', 'practice',
            'method', 'way', 'approach', 'technique', 'strategy', 'tactic', 'plan',
            'scheme', 'program', 'project', 'initiative', 'campaign', 'movement',
            'trend', 'pattern', 'model', 'framework', 'structure', 'system', 'process',
            'procedure', 'operation', 'activity', 'action', 'behavior', 'conduct',
            'performance', 'function', 'role', 'purpose', 'goal', 'objective', 'target',
            'aim', 'intention', 'policy', 'rule', 'regulation', 'law', 'legislation',
            'act', 'bill', 'proposal', 'suggestion', 'recommendation', 'advice',
            'guidance', 'direction', 'instruction', 'order', 'command', 'request',
            'demand', 'requirement', 'condition', 'criteria', 'standard', 'level',
            'degree', 'extent', 'scope', 'range', 'limit', 'boundary', 'border',
            'edge', 'margin', 'space', 'room', 'place', 'location', 'position',
            'point', 'spot', 'site', 'venue', 'setting', 'environment', 'context',
            'situation', 'circumstance', 'condition', 'state', 'status', 'case',
            'instance', 'example', 'sample', 'specimen', 'template', 'format',
            'style', 'type', 'kind', 'sort', 'category', 'class', 'set', 'collection',
            'series', 'sequence', 'chain', 'link', 'connection', 'relation',
            'relationship', 'association', 'bond', 'tie', 'bridge', 'gap', 'distance',
            'difference', 'similarity', 'comparison', 'contrast', 'distinction',
            'separation', 'division', 'split', 'break', 'cut', 'slice', 'piece',
            'part', 'section', 'segment', 'portion', 'fraction', 'percentage',
            'ratio', 'proportion', 'rate', 'speed', 'velocity', 'acceleration',
            'momentum', 'force', 'power', 'energy', 'strength', 'weakness',
            'advantage', 'disadvantage', 'benefit', 'cost', 'price', 'value',
            'worth', 'importance', 'significance', 'meaning', 'reason', 'cause',
            'effect', 'result', 'outcome', 'consequence', 'impact', 'influence',
            'change', 'transformation', 'development', 'growth', 'progress',
            'advancement', 'improvement', 'enhancement', 'upgrade', 'update',
            'revision', 'modification', 'adjustment', 'adaptation', 'accommodation',
            'integration', 'coordination', 'cooperation', 'collaboration',
            'partnership', 'alliance', 'union', 'merger', 'acquisition',
            'combination', 'mixture', 'blend', 'fusion', 'synthesis', 'creation',
            'production', 'manufacture', 'construction', 'building', 'establishment',
            'foundation', 'base', 'basis', 'ground', 'support', 'backing',
            'assistance', 'help', 'aid', 'service', 'facility', 'resource',
            'asset', 'property', 'possession', 'ownership', 'control', 'management',
            'administration', 'governance', 'leadership', 'supervision', 'oversight',
            'monitoring', 'tracking', 'following', 'pursuing', 'chasing', 'hunting',
            'seeking', 'searching', 'looking', 'finding', 'discovering', 'detecting',
            'identifying', 'recognizing', 'understanding', 'comprehending', 'learning',
            'studying', 'researching', 'investigating', 'exploring', 'examining',
            'analyzing', 'evaluating', 'assessing', 'judging', 'deciding', 'choosing',
            'selecting', 'picking', 'opting', 'preferring', 'favoring', 'liking',
            'loving', 'enjoying', 'appreciating', 'valuing', 'treasuring', 'cherishing',
            'respecting', 'admiring', 'praising', 'commending', 'recommending',
            'suggesting', 'proposing', 'offering', 'providing', 'supplying',
            'delivering', 'giving', 'presenting', 'showing', 'displaying', 'exhibiting',
            'demonstrating', 'proving', 'confirming', 'verifying', 'validating',
            'authenticating', 'certifying', 'guaranteeing', 'ensuring', 'securing',
            'protecting', 'defending', 'guarding', 'watching', 'observing',
            'noticing', 'seeing', 'viewing', 'listening', 'hearing', 'feeling',
            'touching', 'tasting', 'smelling', 'sensing', 'perceiving', 'experiencing',
            'undergoing', 'suffering', 'enduring', 'bearing', 'tolerating', 'accepting',
            'receiving', 'getting', 'obtaining', 'acquiring', 'gaining', 'earning',
            'winning', 'achieving', 'accomplishing', 'completing', 'finishing',
            'ending', 'concluding', 'terminating', 'stopping', 'ceasing', 'halting',
            'pausing', 'waiting', 'staying', 'remaining', 'continuing', 'persisting',
            'lasting', 'enduring', 'surviving', 'living', 'existing', 'being',
            'becoming', 'growing', 'developing', 'changing', 'transforming', 'evolving',
            'progressing', 'advancing', 'moving', 'going', 'coming', 'arriving',
            'reaching', 'getting', 'obtaining', 'acquiring', 'gaining', 'earning',
            'winning', 'achieving', 'accomplishing', 'completing', 'finishing',
            'ending', 'concluding', 'terminating', 'stopping', 'ceasing', 'halting',
            'pausing', 'waiting', 'staying', 'remaining', 'continuing', 'persisting',
            'lasting', 'enduring', 'surviving', 'living', 'existing', 'being',
            'becoming', 'growing', 'developing', 'changing', 'transforming', 'evolving',
            'progressing', 'advancing', 'moving', 'going', 'coming', 'arriving', 'reaching'
        }
        
        results = []
        for keyword, count in top_keywords:
            # HTML/ì›¹ ê´€ë ¨ ë¶ˆìš©ì–´ í•„í„°ë§
            if keyword.lower() in html_stopwords:
                continue
                
            # í•´ë‹¹ í‚¤ì›Œë“œì˜ í’ˆì‚¬ ì •ë³´ ì°¾ê¸°
            pos_info = next((kw for kw in all_keywords if kw['keyword'] == keyword), None)
            pos = pos_info['pos'] if pos_info else 'Unknown'
            
            results.append({
                'keyword': keyword,
                'count': count,
                'pos': pos,
                'length': len(keyword)
            })
        
        print(f"âœ… í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: {len(results)}ê°œ")
        return results
    
    def _extract_keywords_basic(self, texts: List[str], max_keywords: int) -> List[Dict]:
        """
        ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ (í˜•íƒœì†Œ ë¶„ì„ ì—†ì´)
        
        Args:
            texts: ë¶„ì„í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            max_keywords: ìµœëŒ€ í‚¤ì›Œë“œ ìˆ˜
            
        Returns:
            List[Dict]: í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        print(f"ğŸ” ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ: {len(texts)}ê°œ í…ìŠ¤íŠ¸")
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        processed_texts = [self.preprocess_text(text) for text in texts]
        processed_texts = [text for text in processed_texts if text]
        
        if not processed_texts:
            return []
        
        # ë‹¨ì–´ ì¶”ì¶œ ë° ë¹ˆë„ ê³„ì‚°
        all_words = []
        for text in processed_texts:
            words = text.split()
            # ë¶ˆìš©ì–´ ì œê±° ë° ê¸¸ì´ í•„í„°ë§
            filtered_words = [
                word for word in words 
                if word not in self.stop_words and len(word) > 1
            ]
            all_words.extend(filtered_words)
        
        # ë¹ˆë„ ê³„ì‚°
        word_freq = Counter(all_words)
        
        # ìƒìœ„ í‚¤ì›Œë“œ ì„ íƒ
        top_keywords = word_freq.most_common(max_keywords)
        
        return [
            {'keyword': word, 'count': count, 'pos': 'Unknown'} 
            for word, count in top_keywords
        ]
    
    def extract_topics_tfidf(self, texts: List[str], n_topics: int = 10) -> List[Dict]:
        """
        TF-IDFë¥¼ ì´ìš©í•œ í† í”½ ì¶”ì¶œ
        
        Args:
            texts: ë¶„ì„í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            n_topics: ì¶”ì¶œí•  í† í”½ ìˆ˜
            
        Returns:
            List[Dict]: í† í”½ ë¦¬ìŠ¤íŠ¸
        """
        if not texts or len(texts) < 2:
            return []
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        processed_texts = [self.preprocess_text(text) for text in texts]
        processed_texts = [text for text in processed_texts if text]
        
        if len(processed_texts) < 2:
            return []
        
        try:
            # TF-IDF ë²¡í„°í™”
            vectorizer = TfidfVectorizer(
                max_features=1000,
                stop_words=None,  # í•œêµ­ì–´ ë¶ˆìš©ì–´ëŠ” ë³„ë„ ì²˜ë¦¬
                ngram_range=(1, 2)  # 1-gramê³¼ 2-gram ì‚¬ìš©
            )
            
            tfidf_matrix = vectorizer.fit_transform(processed_texts)
            feature_names = vectorizer.get_feature_names_out()
            
            # K-means í´ëŸ¬ìŠ¤í„°ë§
            kmeans = KMeans(n_clusters=min(n_topics, len(processed_texts)), random_state=42)
            cluster_labels = kmeans.fit_predict(tfidf_matrix)
            
            # í´ëŸ¬ìŠ¤í„°ë³„ ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ
            topics = []
            for i in range(min(n_topics, len(processed_texts))):
                cluster_indices = np.where(cluster_labels == i)[0]
                if len(cluster_indices) > 0:
                    # í´ëŸ¬ìŠ¤í„° ë‚´ ë¬¸ì„œë“¤ì˜ TF-IDF ì ìˆ˜ í‰ê· 
                    cluster_tfidf = tfidf_matrix[cluster_indices].mean(axis=0).A1
                    
                    # ìƒìœ„ í‚¤ì›Œë“œ ì„ íƒ
                    top_indices = cluster_tfidf.argsort()[-10:][::-1]
                    top_keywords = [feature_names[idx] for idx in top_indices if cluster_tfidf[idx] > 0]
                    
                    if top_keywords:
                        topics.append({
                            'topic_id': i,
                            'keywords': top_keywords,
                            'document_count': len(cluster_indices)
                        })
            
            return topics
            
        except Exception as e:
            print(f"TF-IDF í† í”½ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []
    
    def extract_topics_simple(self, texts, n_topics: int = 10) -> List[Dict]:
        """
        ê°„ë‹¨í•œ í† í”½ ì¶”ì¶œ (ë¹ˆë„ ê¸°ë°˜)
        
        Args:
            texts: ë¶„ì„í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
            n_topics: ì¶”ì¶œí•  í† í”½ ìˆ˜
            
        Returns:
            List[Dict]: í† í”½ ë¦¬ìŠ¤íŠ¸
        """
        if not texts:
            return []
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° 'text' í‚¤ì—ì„œ ì¶”ì¶œ)
        text_list = []
        for item in texts:
            if isinstance(item, dict):
                # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° 'text' ë˜ëŠ” 'text_clean' í‚¤ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text = item.get('text_clean', item.get('text', ''))
                if text:
                    text_list.append(text)
            elif isinstance(item, str):
                text_list.append(item)
        
        if not text_list:
            return []
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = self.extract_keywords(text_list, max_keywords=100)
        
        # ìƒìœ„ í‚¤ì›Œë“œë¥¼ í† í”½ìœ¼ë¡œ ì‚¬ìš©
        topics = []
        for i, kw in enumerate(keywords[:n_topics]):
            topics.append({
                'topic_id': i,
                'topic': kw['keyword'],
                'count': kw['count'],
                'keywords': [kw['keyword']]
            })
        
        return topics
    
    def cluster_keywords(self, keywords: List[Dict], n_clusters: int = 5) -> List[Dict]:
        """
        í‚¤ì›Œë“œ í´ëŸ¬ìŠ¤í„°ë§
        
        Args:
            keywords: í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            n_clusters: í´ëŸ¬ìŠ¤í„° ìˆ˜
            
        Returns:
            List[Dict]: í´ëŸ¬ìŠ¤í„° ë¦¬ìŠ¤íŠ¸
        """
        if not keywords or len(keywords) < n_clusters:
            return []
        
        try:
            # í‚¤ì›Œë“œë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            keyword_texts = [kw['keyword'] for kw in keywords]
            
            # TF-IDF ë²¡í„°í™”
            vectorizer = TfidfVectorizer(
                max_features=100,
                stop_words=None,
                ngram_range=(1, 1)
            )
            
            tfidf_matrix = vectorizer.fit_transform(keyword_texts)
            
            # K-means í´ëŸ¬ìŠ¤í„°ë§
            kmeans = KMeans(n_clusters=min(n_clusters, len(keywords)), random_state=42)
            cluster_labels = kmeans.fit_predict(tfidf_matrix)
            
            # í´ëŸ¬ìŠ¤í„°ë³„ í‚¤ì›Œë“œ ê·¸ë£¹í™”
            clusters = {}
            for i, label in enumerate(cluster_labels):
                if label not in clusters:
                    clusters[label] = []
                clusters[label].append(keywords[i])
            
            # í´ëŸ¬ìŠ¤í„° ê²°ê³¼ ì •ë¦¬
            cluster_results = []
            for cluster_id, cluster_keywords in clusters.items():
                cluster_results.append({
                    'cluster_id': cluster_id,
                    'keywords': cluster_keywords,
                    'size': len(cluster_keywords)
                })
            
            return cluster_results
            
        except Exception as e:
            print(f"í‚¤ì›Œë“œ í´ëŸ¬ìŠ¤í„°ë§ ì˜¤ë¥˜: {e}")
            return []
    
    def compare_topics(self, topics1: List[str], topics2: List[str]) -> Dict[str, Any]:
        """
        ë‘ í† í”½ ì§‘í•© ë¹„êµ
        
        Args:
            topics1: ì²« ë²ˆì§¸ í† í”½ ë¦¬ìŠ¤íŠ¸
            topics2: ë‘ ë²ˆì§¸ í† í”½ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            Dict: ë¹„êµ ê²°ê³¼
        """
        set1 = set(topics1)
        set2 = set(topics2)
        
        common = set1 & set2
        only1 = set1 - set2
        only2 = set2 - set1
        
        return {
            'common_topics': list(common),
            'only_in_first': list(only1),
            'only_in_second': list(only2),
            'common_count': len(common),
            'first_only_count': len(only1),
            'second_only_count': len(only2),
            'similarity': len(common) / len(set1 | set2) if set1 | set2 else 0
        }
    
    def top_topics(self, corpus: List[str], k: int = 10) -> List[str]:
        """
        TF-IDF ê¸°ë°˜ ìƒìœ„ í† í”½ ì¶”ì¶œ (ì“°ë ˆê¸° ë‹¨ì–´ ì œê±° + í’ˆì§ˆ ê°€ë“œ + ë””ë²„ê¹…)
        
        Args:
            corpus: í…ìŠ¤íŠ¸ ì½”í¼ìŠ¤ (text_clean ë¦¬ìŠ¤íŠ¸)
            k: ìƒìœ„ kê°œ í† í”½
            
        Returns:
            List[str]: ìƒìœ„ í† í”½ ë¦¬ìŠ¤íŠ¸
            
        Raises:
            ValueError: í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨
        """
        from src.common.trace import snapshot_df, log_shape
        
        if not corpus:
            print("âŒ í† í”½ ì¶”ì¶œ: ë¹ˆ ì½”í¼ìŠ¤")
            return []
        
        print(f"ğŸ” í† í”½ ì¶”ì¶œ ì‹œì‘: {len(corpus)}ê°œ ë¬¸ì„œ, {k}ê°œ í† í”½ ì¶”ì¶œ")
        
        # ì›ë³¸ ë°ì´í„° ë””ë²„ê¹…
        print("ğŸ“Š ì›ë³¸ ë°ì´í„° ìƒ˜í”Œ:")
        for i, text in enumerate(corpus[:3]):
            print(f"  {i+1}: {text[:100]}...")
        
        # í†µí•©ëœ ë¶ˆìš©ì–´ ì„¸íŠ¸
        combined_stopwords = STOPWORDS | GARBAGE_TOKENS
        print(f"ğŸš« ë¶ˆìš©ì–´ ì„¸íŠ¸ í¬ê¸°: {len(combined_stopwords)}")
        
        # TF-IDF ë²¡í„°ë¼ì´ì € ì„¤ì • (ë™ì  ì¡°ì •)
        corpus_size = len(corpus)
        min_df = min(3, max(1, corpus_size // 10))  # ë™ì  min_df
        max_df = 0.8  # ì™„í™”ëœ max_df
        
        print(f"âš™ï¸ ë²¡í„°ë¼ì´ì € ì„¤ì •: min_df={min_df}, max_df={max_df}")
        
        vectorizer = TfidfVectorizer(
            ngram_range=(1, 2),  # 1-gram, 2-gram
            min_df=min_df,       # ë™ì  min_df
            max_df=max_df,       # ì™„í™”ëœ max_df
            stop_words=list(combined_stopwords),
            token_pattern=r'(?u)\b[ê°€-í£a-zA-Z]{2,}\b'  # ìˆ«ì/í•œê¸€ì ì œì™¸
        )
        
        try:
            # TF-IDF í–‰ë ¬ ìƒì„±
            tfidf_matrix = vectorizer.fit_transform(corpus)
            feature_names = vectorizer.get_feature_names_out()
            
            print(f"ğŸ“ˆ TF-IDF í–‰ë ¬ ìƒì„±: {tfidf_matrix.shape}")
            print(f"ğŸ”¤ ì¶”ì¶œëœ íŠ¹ì„± ìˆ˜: {len(feature_names)}")
            
            if len(feature_names) == 0:
                print("âŒ í† í”½ ì¶”ì¶œ: ìœ íš¨í•œ íŠ¹ì„±ì´ ì—†ìŠµë‹ˆë‹¤")
                # ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œë„
                return self._fallback_keyword_extraction(corpus, k)
            
            # í‰ê·  TF-IDF ì ìˆ˜ ê³„ì‚°
            mean_scores = np.mean(tfidf_matrix.toarray(), axis=0)
            
            # ìƒìœ„ kê°œ í† í”½ ì„ íƒ (ë” ë§ì€ í›„ë³´ í™•ë³´)
            top_indices = np.argsort(mean_scores)[::-1][:k*3]  # ì—¬ìœ ë¶„ í™•ë³´
            top_topics_list = [feature_names[i] for i in top_indices if mean_scores[i] > 0]
            
            print(f"ğŸ¯ ìƒìœ„ í›„ë³´ í† í”½: {len(top_topics_list)}ê°œ")
            print(f"ğŸ“‹ í›„ë³´ í† í”½ ìƒ˜í”Œ: {top_topics_list[:10]}")
            
            # ì“°ë ˆê¸° í† í° í•„í„°ë§ (í’ˆì§ˆ ê°€ë“œ) - ë” ì—„ê²©í•˜ê²Œ
            filtered_topics = []
            rejected_topics = []
            
            for topic in top_topics_list:
                # ì“°ë ˆê¸° í† í°ì´ í¬í•¨ë˜ì§€ ì•Šì€ ê²½ìš°ë§Œ ì¶”ê°€
                if not any(garbage in topic.lower() for garbage in GARBAGE_TOKENS):
                    # ì¶”ê°€ í’ˆì§ˆ ê²€ì¦
                    if (len(topic) > 1 and 
                        not topic.isdigit() and 
                        not topic.lower() in ['ai', 'rss', 'xml', 'json'] and
                        not any(char.isdigit() for char in topic)):
                        filtered_topics.append(topic)
                    else:
                        rejected_topics.append(topic)
                else:
                    rejected_topics.append(topic)
            
            print(f"âœ… í•„í„°ë§ëœ í† í”½: {len(filtered_topics)}ê°œ")
            print(f"âŒ ê±°ë¶€ëœ í† í”½: {len(rejected_topics)}ê°œ")
            if rejected_topics:
                print(f"ğŸš« ê±°ë¶€ëœ í† í”½ ìƒ˜í”Œ: {rejected_topics[:10]}")
            
            # ìµœì¢… í’ˆì§ˆ ê²€ì¦
            final_topics = filtered_topics[:k]
            
            # ê¸ˆì§€ í† í° ì¬ê²€ì¦ (ë” ê°•í™”ëœ ë²„ì „)
            forbidden_tokens = [
                'nbsp', 'font', 'href', 'https', 'com', 'google', 'news', 'rss', 'xml', 'json', 'api',
                'articles', 'target', 'oc', 'feed', 'atom', 'syndication', 'channel', 'item',
                'link', 'description', 'pubdate', 'guid', 'category', 'enclosure', 'www', 'http'
            ]
            final_filtered_topics = []
            
            for topic in final_topics:
                # í† í°ë³„ë¡œ ë¶„ë¦¬í•´ì„œ ê²€ì‚¬
                topic_tokens = topic.lower().split()
                has_forbidden = False
                
                for token in topic_tokens:
                    if token in forbidden_tokens:
                        print(f"âš ï¸ ìµœì¢… ê²€ì¦ì—ì„œ ê±°ë¶€: '{topic}' (ê¸ˆì§€ í† í°: '{token}')")
                        has_forbidden = True
                        break
                
                if not has_forbidden:
                    final_filtered_topics.append(topic)
            
            # ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë©´ fallback ì‚¬ìš©
            if len(final_filtered_topics) < k // 2:
                print("âš ï¸ í’ˆì§ˆ ë†’ì€ í† í”½ì´ ë¶€ì¡±í•˜ì—¬ fallback ë°©ì‹ ì‚¬ìš©")
                return self._fallback_keyword_extraction(corpus, k)
            
            # ê²°ê³¼ ì €ì¥
            topics_df = pd.DataFrame({"topic": final_filtered_topics})
            snapshot_df(topics_df, "topics_top10")
            
            print(f"âœ… í† í”½ ì¶”ì¶œ ì™„ë£Œ: {len(final_filtered_topics)}ê°œ")
            print(f"ğŸ¯ ìµœì¢… í† í”½: {final_filtered_topics}")
            return final_filtered_topics
            
        except Exception as e:
            print(f"âŒ í† í”½ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            print("ğŸ”„ Fallback ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„")
            return self._fallback_keyword_extraction(corpus, k)
    
    def _fallback_keyword_extraction(self, corpus: List[str], k: int) -> List[str]:
        """
        Fallback í‚¤ì›Œë“œ ì¶”ì¶œ ë°©ì‹ (TF-IDF ì‹¤íŒ¨ ì‹œ)
        
        Args:
            corpus: í…ìŠ¤íŠ¸ ì½”í¼ìŠ¤
            k: ì¶”ì¶œí•  í‚¤ì›Œë“œ ìˆ˜
            
        Returns:
            List[str]: í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        print("ğŸ”„ Fallback í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘")
        
        try:
            # ê°„ë‹¨í•œ ë¹ˆë„ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
            all_words = []
            for text in corpus:
                if text:
                    # í† í°í™” ë° í•„í„°ë§
                    tokens = text.lower().split()
                    filtered_tokens = [
                        token for token in tokens 
                        if (len(token) > 1 and 
                            token not in STOPWORDS and 
                            token not in GARBAGE_TOKENS and
                            not token.isdigit() and
                            not any(char.isdigit() for char in token))
                    ]
                    all_words.extend(filtered_tokens)
            
            if not all_words:
                print("âŒ Fallback: ìœ íš¨í•œ ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤")
                return []
            
            # ë¹ˆë„ ê³„ì‚°
            from collections import Counter
            word_freq = Counter(all_words)
            
            # ìƒìœ„ í‚¤ì›Œë“œ ì„ íƒ
            top_keywords = [word for word, count in word_freq.most_common(k*2)]
            
            # ìµœì¢… í•„í„°ë§
            final_keywords = []
            for keyword in top_keywords:
                if (len(keyword) > 1 and 
                    keyword not in ['ai', 'rss', 'xml', 'json', 'api', 'http', 'www'] and
                    not any(char.isdigit() for char in keyword)):
                    final_keywords.append(keyword)
            
            result = final_keywords[:k]
            print(f"âœ… Fallback ì™„ë£Œ: {len(result)}ê°œ í‚¤ì›Œë“œ")
            print(f"ğŸ¯ Fallback ê²°ê³¼: {result}")
            return result
            
        except Exception as e:
            print(f"âŒ Fallback ì‹¤íŒ¨: {e}")
            return []
    
    def doc_topics(self, text: str, n: int = 5) -> List[str]:
        """
        ë‹¨ì¼ ë¬¸ì„œì˜ ì£¼ìš” í† í”½ ì¶”ì¶œ
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            n: ìƒìœ„ nê°œ í† í”½
            
        Returns:
            List[str]: ì£¼ìš” í† í”½ ë¦¬ìŠ¤íŠ¸
        """
        if not text:
            return []
        
        # ê°„ë‹¨í•œ í† í°í™” ë° ë¹ˆë„ ë¶„ì„
        tokens = text.lower().split()
        
        # ë¶ˆìš©ì–´ ë° ì“°ë ˆê¸° í† í° ì œê±°
        combined_stopwords = STOPWORDS | GARBAGE_TOKENS
        filtered_tokens = [
            token for token in tokens 
            if len(token) > 1 and token not in combined_stopwords
        ]
        
        if not filtered_tokens:
            return []
        
        # ë¹ˆë„ ê³„ì‚°
        from collections import Counter
        token_counts = Counter(filtered_tokens)
        
        # ìƒìœ„ nê°œ ë°˜í™˜
        return [token for token, count in token_counts.most_common(n)]
