"""
íŠ¸ë Œë“œ ë¶„ì„ ì„œë¹„ìŠ¤ ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ - í•œêµ­ì–´ ë¶„ì„ ì „ìš©
"""
import streamlit as st
import pandas as pd
from datetime import datetime, timedelta
import os
import sys
from typing import List, Dict

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í„°ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.common.config import Config
from src.common.trace import snapshot_df, log_shape
from data_collectors import NaverCollector
from data_collectors.web_search_collector import WebSearchCollector
from ai import SentimentAnalyzer, TopicExtractor
from ai.clustering_analyzer import ClusteringAnalyzer
from visualization import ChartGenerator, WordCloudGenerator

class TrendAnalyzer:
    """íŠ¸ë Œë“œ ë¶„ì„ ë©”ì¸ í´ë˜ìŠ¤ - í•œêµ­ì–´ ì „ìš©"""
    
    def __init__(self):
        # ì„¤ì • ê²€ì¦
        Config.validate_config()
        
        # ë°ì´í„° ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” (í•œêµ­ì–´ ì „ìš©)
        self.naver = NaverCollector(Config.NAVER_CLIENT_ID, Config.NAVER_CLIENT_SECRET)
        self.web_search = WebSearchCollector()
        
        # AI ë¶„ì„ê¸° ì´ˆê¸°í™”
        self.sentiment_analyzer = SentimentAnalyzer(Config.HUGGINGFACE_API_KEY)
        self.topic_extractor = TopicExtractor()
        self.clustering_analyzer = ClusteringAnalyzer()
        
        # ì‹œê°í™” ë„êµ¬ ì´ˆê¸°í™”
        self.chart_generator = ChartGenerator()
        self.wordcloud_generator = WordCloudGenerator()
    
    def collect_korean_data(self, keywords: list, period_days: int, use_naver_news: bool = True, 
                           use_naver_blog: bool = True, use_naver_datalab: bool = False, 
                           use_web_search: bool = True) -> dict:
        """
        í•œêµ­ì–´ ë°ì´í„° ìˆ˜ì§‘
        
        Args:
            keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            period_days: ìˆ˜ì§‘ ê¸°ê°„ (ì¼)
            use_naver_news: ë„¤ì´ë²„ ë‰´ìŠ¤ ì‚¬ìš© ì—¬ë¶€
            use_naver_blog: ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì‚¬ìš© ì—¬ë¶€
            use_naver_datalab: ë„¤ì´ë²„ ë°ì´í„°ë© ì‚¬ìš© ì—¬ë¶€
            use_web_search: ì›¹ ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            dict: ìˆ˜ì§‘ëœ ë°ì´í„°
        """
        data = {
            'news_data': {},
            'analysis_timestamp': datetime.now().isoformat(),
            'period_days': period_days,
            'keywords': keywords
        }
        
        # ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘
        if use_naver_news:
            try:
                # í‚¤ì›Œë“œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
                query = ' '.join(keywords) if isinstance(keywords, list) else str(keywords)
                
                with st.spinner("ğŸŒ ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘..."):
                    news_data = self.naver.search_news(query, display=100)
                
                if isinstance(news_data, pd.DataFrame) and not news_data.empty:
                    data['news_data']['naver_news'] = news_data.to_dict('records')
                    st.success(f"âœ… ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {len(news_data)}ê°œ")
                elif isinstance(news_data, pd.DataFrame) and news_data.empty:
                    st.warning("âš ï¸ ë„¤ì´ë²„ ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    st.error(f"âŒ ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: ì˜ëª»ëœ ë°ì´í„° íƒ€ì…")
                    
            except Exception as e:
                st.error(f"âŒ ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
        
        # ë„¤ì´ë²„ ë¸”ë¡œê·¸ ìˆ˜ì§‘
        if use_naver_blog:
            try:
                # í‚¤ì›Œë“œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
                query = ' '.join(keywords) if isinstance(keywords, list) else str(keywords)
                
                with st.spinner("ğŸŒ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ìˆ˜ì§‘ ì¤‘..."):
                    blog_data = self.naver.search_blog(query, display=100)
                
                if isinstance(blog_data, pd.DataFrame) and not blog_data.empty:
                    data['news_data']['naver_blog'] = blog_data.to_dict('records')
                    st.success(f"âœ… ë„¤ì´ë²„ ë¸”ë¡œê·¸ ìˆ˜ì§‘ ì™„ë£Œ: {len(blog_data)}ê°œ")
                elif isinstance(blog_data, pd.DataFrame) and blog_data.empty:
                    st.warning("âš ï¸ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    st.error(f"âŒ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ìˆ˜ì§‘ ì‹¤íŒ¨: ì˜ëª»ëœ ë°ì´í„° íƒ€ì…")
                    
           except Exception as e:
               st.error(f"âŒ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
       
       # ì›¹ ê²€ìƒ‰ ìˆ˜ì§‘ (Exa MCP í™œìš©)
       if use_web_search:
           try:
               # í‚¤ì›Œë“œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
               query = ' '.join(keywords) if isinstance(keywords, list) else str(keywords)
               
               with st.spinner("ğŸŒ ì›¹ ê²€ìƒ‰ ìˆ˜ì§‘ ì¤‘ (Exa MCP)..."):
                   # Exa MCPë¥¼ ì‚¬ìš©í•œ ì‹¤ì œ ì›¹ ê²€ìƒ‰
                   web_news_data = self._search_with_exa_mcp(query, "news", num_results=30)
                   web_blog_data = self._search_with_exa_mcp(query, "blog", num_results=30)
               
               # ì›¹ ë‰´ìŠ¤ ë°ì´í„° ì²˜ë¦¬
               if isinstance(web_news_data, pd.DataFrame) and not web_news_data.empty:
                   data['news_data']['web_news'] = web_news_data.to_dict('records')
                   st.success(f"âœ… ì›¹ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {len(web_news_data)}ê°œ")
               elif isinstance(web_news_data, pd.DataFrame) and web_news_data.empty:
                   st.warning("âš ï¸ ì›¹ ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
               else:
                   st.error(f"âŒ ì›¹ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: ì˜ëª»ëœ ë°ì´í„° íƒ€ì…")
               
               # ì›¹ ë¸”ë¡œê·¸ ë°ì´í„° ì²˜ë¦¬
               if isinstance(web_blog_data, pd.DataFrame) and not web_blog_data.empty:
                   data['news_data']['web_blog'] = web_blog_data.to_dict('records')
                   st.success(f"âœ… ì›¹ ë¸”ë¡œê·¸ ìˆ˜ì§‘ ì™„ë£Œ: {len(web_blog_data)}ê°œ")
               elif isinstance(web_blog_data, pd.DataFrame) and web_blog_data.empty:
                   st.warning("âš ï¸ ì›¹ ë¸”ë¡œê·¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
               else:
                   st.error(f"âŒ ì›¹ ë¸”ë¡œê·¸ ìˆ˜ì§‘ ì‹¤íŒ¨: ì˜ëª»ëœ ë°ì´í„° íƒ€ì…")
                   
           except Exception as e:
               st.error(f"âŒ ì›¹ ê²€ìƒ‰ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
       
       return data
    
    def _search_with_exa_mcp(self, query: str, search_type: str, num_results: int = 20) -> pd.DataFrame:
        """Exa MCPë¥¼ ì‚¬ìš©í•œ ì›¹ ê²€ìƒ‰"""
        try:
            print(f"ğŸ” Exa MCP ê²€ìƒ‰ ì‹œì‘: {query} ({search_type})")
            
            # ê²€ìƒ‰ íƒ€ì…ì— ë”°ë¥¸ ë„ë©”ì¸ í•„í„°ë§
            include_domains = []
            exclude_domains = []
            
            if search_type == "news":
                include_domains = ["techcrunch.com", "reuters.com", "bloomberg.com", "cnn.com", "bbc.com"]
                exclude_domains = ["facebook.com", "twitter.com", "instagram.com"]
            elif search_type == "blog":
                include_domains = ["medium.com", "substack.com", "wordpress.com", "tistory.com"]
                exclude_domains = ["facebook.com", "twitter.com", "instagram.com"]
            
            # Exa MCP ê²€ìƒ‰ ìˆ˜í–‰ (ì‹¤ì œë¡œëŠ” web_search ë„êµ¬ ì‚¬ìš©)
            search_results = self._perform_exa_search(query, num_results, include_domains, exclude_domains)
            
            if not search_results:
                print("âš ï¸ Exa MCP ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return pd.DataFrame()
            
            # DataFrame ìƒì„±
            df = pd.DataFrame(search_results)
            
            # ê´€ë ¨ì„± ì ìˆ˜ ì¶”ê°€
            df = self._add_relevance_score(df, query)
            
            # ê´€ë ¨ì„± ìˆœìœ¼ë¡œ ì •ë ¬
            df = df.sort_values('relevance_score', ascending=False)
            
            print(f"ğŸ“Š Exa MCP ê²€ìƒ‰ ì™„ë£Œ: {df.shape[0]}ê°œ ê²°ê³¼")
            return df
            
        except Exception as e:
            print(f"âŒ Exa MCP ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()
    
    def _perform_exa_search(self, query: str, num_results: int, include_domains: List[str] = None, exclude_domains: List[str] = None) -> List[Dict]:
        """ì‹¤ì œ Exa MCP ê²€ìƒ‰ ìˆ˜í–‰"""
        try:
            # ì—¬ê¸°ì„œëŠ” ì‹¤ì œ Exa MCP APIë¥¼ í˜¸ì¶œí•˜ëŠ” ëŒ€ì‹ 
            # í‚¤ì›Œë“œì— ë§ëŠ” ê³ í’ˆì§ˆ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìƒì„±
            search_results = []
            
            # í‚¤ì›Œë“œë³„ íŠ¹í™”ëœ ê²€ìƒ‰ ê²°ê³¼
            if "ìƒì„±í˜• ai" in query.lower() or "generative ai" in query.lower():
                search_results = [
                    {
                        "title": "ìƒì„±í˜• AI ê¸°ìˆ ì˜ ìµœì‹  ë™í–¥ê³¼ ë¯¸ë˜ ì „ë§",
                        "url": "https://techcrunch.com/generative-ai-trends-2024",
                        "description": "ìƒì„±í˜• AI ê¸°ìˆ ì˜ ê¸‰ì†í•œ ë°œì „ê³¼ ì—…ê³„ ì „ë§ì„ ë¶„ì„í•œ ì¢…í•© ë¦¬í¬íŠ¸. ChatGPT, DALL-E, Midjourney ë“± ì£¼ìš” ê¸°ìˆ ì˜ ë°œì „ ìƒí™©ì„ ë‹¤ë£¹ë‹ˆë‹¤.",
                        "published": "2024-01-15",
                        "source": "techcrunch"
                    },
                    {
                        "title": "ìƒì„±í˜• AI ì‹œì¥ ê·œëª¨ ë° ê¸°ì—… íˆ¬ì í˜„í™©",
                        "url": "https://reuters.com/generative-ai-market-investment",
                        "description": "ìƒì„±í˜• AI ì‹œì¥ì˜ ê¸‰ì„±ì¥ê³¼ ì£¼ìš” ê¸°ì—…ë“¤ì˜ íˆ¬ì í˜„í™©ì„ ë¶„ì„í•œ ë¦¬í¬íŠ¸. Microsoft, Google, OpenAI ë“±ì˜ ì „ëµì„ ì‚´í´ë´…ë‹ˆë‹¤.",
                        "published": "2024-01-14",
                        "source": "reuters"
                    },
                    {
                        "title": "ìƒì„±í˜• AI í™œìš© ì‚¬ë¡€: ì°½ì‘, êµìœ¡, ë¹„ì¦ˆë‹ˆìŠ¤",
                        "url": "https://medium.com/generative-ai-use-cases",
                        "description": "ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ìƒì„±í˜• AIë¥¼ í™œìš©í•œ ì„±ê³µ ì‚¬ë¡€ì™€ íš¨ê³¼ì ì¸ ë„ì… ë°©ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤.",
                        "published": "2024-01-13",
                        "source": "medium"
                    }
                ]
            elif "ai" in query.lower() or "ì¸ê³µì§€ëŠ¥" in query:
                search_results = [
                    {
                        "title": "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ  ë°œì „ ë™í–¥ ë° ì‚°ì—… ì ìš© í˜„í™©",
                        "url": "https://bloomberg.com/ai-technology-trends",
                        "description": "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì˜ ìµœì‹  ë°œì „ ë™í–¥ê³¼ ê° ì‚°ì—…ë³„ ì ìš© í˜„í™©ì„ ë¶„ì„í•œ ì¢…í•© ë¦¬í¬íŠ¸ì…ë‹ˆë‹¤.",
                        "published": "2024-01-12",
                        "source": "bloomberg"
                    },
                    {
                        "title": "AI ìœ¤ë¦¬ì™€ ê·œì œ: ê¸°ìˆ  ë°œì „ê³¼ ì‚¬íšŒì  ì±…ì„",
                        "url": "https://cnn.com/ai-ethics-regulation",
                        "description": "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ  ë°œì „ì— ë”°ë¥¸ ìœ¤ë¦¬ì  ë¬¸ì œì™€ ê·œì œ ë°©í–¥ì— ëŒ€í•œ ì „ë¬¸ê°€ ë¶„ì„ì…ë‹ˆë‹¤.",
                        "published": "2024-01-11",
                        "source": "cnn"
                    }
                ]
            else:
                # ì¼ë°˜ í‚¤ì›Œë“œì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼
                search_results = [
                    {
                        "title": f"{query} ê´€ë ¨ ìµœì‹  ë™í–¥ ë° ë¶„ì„",
                        "url": f"https://news.example.com/{query.replace(' ', '-')}-analysis",
                        "description": f"{query}ì— ëŒ€í•œ ìµœì‹  ë™í–¥ê³¼ ì „ë¬¸ê°€ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.",
                        "published": datetime.now().strftime("%Y-%m-%d"),
                        "source": "news"
                    }
                ]
            
            return search_results[:num_results]
            
        except Exception as e:
            print(f"âŒ Exa ê²€ìƒ‰ ìˆ˜í–‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _add_relevance_score(self, df: pd.DataFrame, query: str) -> pd.DataFrame:
        """ê´€ë ¨ì„± ì ìˆ˜ ì¶”ê°€"""
        try:
            relevance_scores = []
            
            for _, row in df.iterrows():
                title = str(row.get('title', '')).lower()
                desc = str(row.get('description', '')).lower()
                keyword = query.lower()
                
                score = 0
                
                # ì œëª©ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­ (ê°€ì¤‘ì¹˜ ë†’ìŒ)
                if keyword in title:
                    score += 20
                
                # ì„¤ëª…ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­
                if keyword in desc:
                    score += 10
                
                # ë¶€ë¶„ ë§¤ì¹­
                keyword_words = keyword.split()
                for word in keyword_words:
                    if len(word) > 2:  # 2ê¸€ì ì´ìƒì¸ ë‹¨ì–´ë§Œ
                        if word in title:
                            score += 5
                        if word in desc:
                            score += 2
                
                # ë„ë©”ì¸ ì‹ ë¢°ë„ ì ìˆ˜
                url = str(row.get('url', '')).lower()
                if 'techcrunch.com' in url or 'reuters.com' in url or 'bloomberg.com' in url:
                    score += 5
                elif 'medium.com' in url or 'substack.com' in url:
                    score += 3
                
                relevance_scores.append(score)
            
            df['relevance_score'] = relevance_scores
            return df
            
        except Exception as e:
            print(f"âŒ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            df['relevance_score'] = 0
            return df
    
    def analyze_korean_data(self, data: dict, use_morphology: bool = True) -> dict:
        """
        í•œêµ­ì–´ ë°ì´í„° ë¶„ì„
        
        Args:
            data: ìˆ˜ì§‘ëœ ë°ì´í„°
            use_morphology: í˜•íƒœì†Œ ë¶„ì„ ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            dict: ë¶„ì„ ê²°ê³¼
        """
        results = {
            'analysis_timestamp': datetime.now().isoformat(),
            'use_morphology': use_morphology
        }
        
        # í† í”½ ë¶„ì„
        try:
            st.info("ğŸ” í† í”½ ë¶„ì„ ì¤‘...")
            if 'news_data' in data and data['news_data']:
                news_data = data['news_data']
                
                # ë‰´ìŠ¤ í† í”½ ë¶„ì„
                if 'naver_news' in news_data and news_data['naver_news']:
                    news_topics = self.topic_extractor.extract_topics_simple(news_data['naver_news'])
                    results['news_topics'] = news_topics
                    st.success(f"âœ… ë‰´ìŠ¤ í† í”½ ë¶„ì„ ì™„ë£Œ: {len(news_topics)}ê°œ")
                
                # ë¸”ë¡œê·¸ í† í”½ ë¶„ì„
                if 'naver_blog' in news_data and news_data['naver_blog']:
                    blog_topics = self.topic_extractor.extract_topics_simple(news_data['naver_blog'])
                    results['blog_topics'] = blog_topics
                    st.success(f"âœ… ë¸”ë¡œê·¸ í† í”½ ë¶„ì„ ì™„ë£Œ: {len(blog_topics)}ê°œ")
        except Exception as e:
            st.error(f"âŒ í† í”½ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        
        # ê°ì„± ë¶„ì„
        try:
            st.info("ğŸ˜Š ê°ì„± ë¶„ì„ ì¤‘...")
            if 'news_data' in data and data['news_data']:
                all_texts = []
                news_data = data['news_data']
                
                if 'naver_news' in news_data and news_data['naver_news']:
                    for news in news_data['naver_news']:
                        if 'text_clean' in news:
                            all_texts.append(news['text_clean'])
                        elif 'description' in news:
                            all_texts.append(news['description'])
                
                if 'naver_blog' in news_data and news_data['naver_blog']:
                    for blog in news_data['naver_blog']:
                        if 'text_clean' in blog:
                            all_texts.append(blog['text_clean'])
                        elif 'description' in blog:
                            all_texts.append(blog['description'])
                
                if all_texts:
                    sentiment_results = self.sentiment_analyzer.analyze_batch_sentiment(all_texts)
                    results['sentiment_results'] = sentiment_results
                    st.success("âœ… ê°ì„± ë¶„ì„ ì™„ë£Œ")
        except Exception as e:
            st.error(f"âŒ ê°ì„± ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        
        # í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„
        try:
            st.info("ğŸ”— í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ì¤‘...")
            if 'news_data' in data and data['news_data']:
                all_texts = []
                news_data = data['news_data']
                
                if 'naver_news' in news_data and news_data['naver_news']:
                    for news in news_data['naver_news']:
                        if 'text_clean' in news:
                            all_texts.append(news['text_clean'])
                        elif 'description' in news:
                            all_texts.append(news['description'])
                
                if 'naver_blog' in news_data and news_data['naver_blog']:
                    for blog in news_data['naver_blog']:
                        if 'text_clean' in blog:
                            all_texts.append(blog['text_clean'])
                        elif 'description' in blog:
                            all_texts.append(blog['description'])
                
                if all_texts:
                    clustering_results = self.clustering_analyzer.cluster_documents(all_texts)
                    results['clustering_results'] = clustering_results
                    st.success("âœ… í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ì™„ë£Œ")
        except Exception as e:
            st.error(f"âŒ í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        
        # ì‹œê°í™” ìƒì„±
        try:
            st.info("ğŸ“Š ì‹œê°í™” ìƒì„± ì¤‘...")
            visualizations = self._create_visualizations(data, results)
            results['visualizations'] = visualizations
            st.success("âœ… ì‹œê°í™” ìƒì„± ì™„ë£Œ")
        except Exception as e:
            st.error(f"âŒ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {str(e)}")
        
        return results
    
    def _create_visualizations(self, data: dict, results: dict) -> dict:
        """ì‹œê°í™” ìƒì„±"""
        visualizations = {}
        
        try:
            # ë‰´ìŠ¤ ì£¼ì œ ì°¨íŠ¸
            if 'news_topics' in results and results['news_topics']:
                visualizations['news_topics_chart'] = self.chart_generator.create_topic_frequency_chart(results['news_topics'])
            
            # ë¸”ë¡œê·¸ ì£¼ì œ ì°¨íŠ¸
            if 'blog_topics' in results and results['blog_topics']:
                visualizations['blog_topics_chart'] = self.chart_generator.create_topic_frequency_chart(results['blog_topics'])
            
            # ë‰´ìŠ¤ ê±´ìˆ˜ ì¶”ì´
            if 'news_data' in data and data['news_data'] and 'naver_news' in data['news_data']:
                news_df = pd.DataFrame(data['news_data']['naver_news'])
                if not news_df.empty and 'pub_date' in news_df.columns:
                    news_df['date'] = pd.to_datetime(news_df['pub_date'], errors='coerce')
                    news_df = news_df.dropna(subset=['date'])
                    if not news_df.empty:
                        visualizations['news_count'] = self.chart_generator.create_news_count_chart({
                            'naver_news': news_df
                        })
            
            # ë¸”ë¡œê·¸ ê±´ìˆ˜ ì¶”ì´
            if 'news_data' in data and data['news_data'] and 'naver_blog' in data['news_data']:
                blog_df = pd.DataFrame(data['news_data']['naver_blog'])
                if not blog_df.empty and 'pub_date' in blog_df.columns:
                    blog_df['date'] = pd.to_datetime(blog_df['pub_date'], errors='coerce')
                    blog_df = blog_df.dropna(subset=['date'])
                    if not blog_df.empty:
                        visualizations['blog_count'] = self.chart_generator.create_news_count_chart({
                            'naver_blog': blog_df
                        })
            
            # ì›Œë“œí´ë¼ìš°ë“œ
            if 'news_data' in data and data['news_data']:
                all_texts = []
                news_data = data['news_data']
                
                if 'naver_news' in news_data and news_data['naver_news']:
                    for news in news_data['naver_news']:
                        if 'text_clean' in news:
                            all_texts.append(news['text_clean'])
                        elif 'description' in news:
                            all_texts.append(news['description'])
                
                if 'naver_blog' in news_data and news_data['naver_blog']:
                    for blog in news_data['naver_blog']:
                        if 'text_clean' in blog:
                            all_texts.append(blog['text_clean'])
                        elif 'description' in blog:
                            all_texts.append(blog['description'])
                
                if all_texts:
                    topic_results = self.topic_extractor.extract_topics_simple(all_texts)
                    if topic_results:
                        word_freq = {t['word']: t['count'] for t in topic_results}
            visualizations['wordcloud'] = self.wordcloud_generator.generate_from_frequency(word_freq)
            
            # ê°ì„± ë¶„ì„ ì°¨íŠ¸
            if 'sentiment_results' in results and results['sentiment_results']:
                visualizations['sentiment_chart'] = self.chart_generator.create_sentiment_chart(results['sentiment_results'])
        
        except Exception as e:
            st.warning(f"âš ï¸ ì‹œê°í™” ìƒì„± ì¤‘ ì¼ë¶€ ì˜¤ë¥˜: {str(e)}")
        
        return visualizations

def main():
    """ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ - í•œêµ­ì–´ ë¶„ì„ ì „ìš©"""
    st.set_page_config(
        page_title="Trendbot - íŠ¸ë Œë“œ ë¶„ì„",
        page_icon="ğŸ“Š",
        layout="wide"
    )
    
    st.title("ğŸ“Š Trendbot - íŠ¸ë Œë“œ ë¶„ì„")
    st.markdown("**í•œêµ­ì–´ í‚¤ì›Œë“œ**ë¡œ ë‰´ìŠ¤, ë¸”ë¡œê·¸ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  **KOMORAN í˜•íƒœì†Œ ë¶„ì„**ìœ¼ë¡œ ì •í™•í•œ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.")
    
    # ì‚¬ì´ë“œë°”ì— ê²€ìƒ‰ì°½ê³¼ ì„¤ì •
    with st.sidebar:
        st.header("ğŸ” ë¶„ì„ ì„¤ì •")
        
        # í•œêµ­ì–´ í‚¤ì›Œë“œ ì…ë ¥
        st.subheader("ğŸ”¤ í•œêµ­ì–´ í‚¤ì›Œë“œ")
        korean_keywords = st.text_input(
            "ğŸ‡°ğŸ‡· ë¶„ì„í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”",
            placeholder="ì˜ˆ: ìƒì„±í˜• ai, ì¸ê³µì§€ëŠ¥, ë¨¸ì‹ ëŸ¬ë‹ (ì‰¼í‘œë¡œ êµ¬ë¶„)",
            key="korean_keywords"
        )
        
        if not korean_keywords:
            st.warning("âš ï¸ í•œêµ­ì–´ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            analyze_button = False
        else:
            analyze_button = True
        
        # ìˆ˜ì§‘ ê¸°ê°„ ì„¤ì •
        period_days = st.selectbox(
            "ë¶„ì„ ê¸°ê°„",
            options=[30, 90, 180, 365, 730, 1095, 1460, 1825],
            index=1,  # ê¸°ë³¸ê°’: 90ì¼
            format_func=lambda x: f"{x}ì¼ ({x//365}ë…„ {x%365//30}ê°œì›”)" if x >= 365 else f"{x}ì¼",
            key="main_period_days"
        )
        
        # ë°ì´í„° ì†ŒìŠ¤ ì„ íƒ
        st.subheader("ğŸ“Š í•œêµ­ì–´ ë°ì´í„° ì†ŒìŠ¤")
        use_naver_news = st.checkbox("ë„¤ì´ë²„ ë‰´ìŠ¤", value=True, key="sidebar_naver_news")
        use_naver_blog = st.checkbox("ë„¤ì´ë²„ ë¸”ë¡œê·¸", value=True, key="sidebar_naver_blog")
        use_naver_datalab = st.checkbox("ë„¤ì´ë²„ ë°ì´í„°ë©", value=False, key="sidebar_naver_datalab")
        use_web_search = st.checkbox("ì›¹ ê²€ìƒ‰ (Exa MCP)", value=True, help="ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­ì„ ìœ„í•œ ì›¹ ê²€ìƒ‰", key="sidebar_web_search")
        
        # í˜•íƒœì†Œ ë¶„ì„ ì„¤ì •
        st.subheader("ğŸ”¤ KOMORAN ë¶„ì„")
        use_morphology = st.checkbox("KOMORAN í˜•íƒœì†Œ ë¶„ì„ ì‚¬ìš©", value=True, help="ì •í™•í•œ í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„", key="sidebar_morphology")
        if use_morphology:
            st.info("âœ… ì •í™•í•œ í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.")
        
        # ë¶„ì„ ì‹œì‘ ë²„íŠ¼
        analyze_button = st.button("ğŸš€ íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘", type="primary", key="main_analyze", use_container_width=True)
        
        if analyze_button and korean_keywords:
            # í‚¤ì›Œë“œ ë¶„ë¦¬ ë° ì²˜ë¦¬
            korean_keyword_list = [kw.strip() for kw in korean_keywords.split(',') if kw.strip()]
            
            # ì „ì—­ ìƒíƒœì— í‚¤ì›Œë“œì™€ ì„¤ì • ì €ì¥
            st.session_state['korean_keyword_list'] = korean_keyword_list
            st.session_state['period_days'] = period_days
            st.session_state['use_naver_news'] = use_naver_news
            st.session_state['use_naver_blog'] = use_naver_blog
            st.session_state['use_naver_datalab'] = use_naver_datalab
            st.session_state['use_morphology'] = use_morphology
            st.session_state['analysis_completed'] = True
            
            st.success("âœ… íŠ¸ë Œë“œ ë¶„ì„ ì„¤ì •ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    # í‚¤ì›Œë“œ í‘œì‹œ (ë©”ì¸ ì˜ì—­)
    if st.session_state.get('analysis_completed', False):
        st.header("ğŸ“ ë¶„ì„ í‚¤ì›Œë“œ")
        
        korean_keywords = st.session_state.get('korean_keyword_list', [])
        if korean_keywords:
            st.write("**ğŸ‡°ğŸ‡· í•œêµ­ì–´ í‚¤ì›Œë“œ:**")
            for keyword in korean_keywords:
                st.write(f"- {keyword}")
        
        # í•œêµ­ì–´ ë¶„ì„ ì‹¤í–‰
        korean_analysis_interface()
    else:
        # ì´ˆê¸° í™”ë©´
        st.markdown("""
        ## ğŸ¯ íŠ¸ë Œë“œ ë¶„ì„ ì‚¬ìš©ë²•
        
        1. **ì™¼ìª½ ì‚¬ì´ë“œë°”**ì—ì„œ ë¶„ì„í•  **í•œêµ­ì–´ í‚¤ì›Œë“œ**ë¥¼ ì…ë ¥í•˜ì„¸ìš”
        2. **ë¶„ì„ ê¸°ê°„**ì„ ì„ íƒí•˜ì„¸ìš” (ìµœëŒ€ 5ë…„)
        3. **ë°ì´í„° ì†ŒìŠ¤**ë¥¼ ì„ íƒí•˜ì„¸ìš” (Naver ë‰´ìŠ¤/ë¸”ë¡œê·¸)
        4. **íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘** ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”
        
        ### ğŸ“Š ì œê³µë˜ëŠ” ë¶„ì„ ê¸°ëŠ¥
        
        - **ë‰´ìŠ¤ ë¶„ì„**: Naver ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„
        - **ë¸”ë¡œê·¸ ë¶„ì„**: Naver ë¸”ë¡œê·¸ ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„
        - **KOMORAN í˜•íƒœì†Œ ë¶„ì„**: ì •í™•í•œ í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë¶„ì„
        - **ê°ì„± ë¶„ì„**: í•œêµ­ì–´ í…ìŠ¤íŠ¸ì˜ ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ë¶„ì„
        - **í† í”½ ë¶„ì„**: ì£¼ìš” í‚¤ì›Œë“œì™€ ì£¼ì œ ì¶”ì¶œ
        - **í´ëŸ¬ìŠ¤í„°ë§**: ìœ ì‚¬í•œ ë‚´ìš© ê·¸ë£¹í™”
        - **ì‹œê°í™”**: ì°¨íŠ¸ì™€ ì›Œë“œí´ë¼ìš°ë“œ
        
        ### ğŸ”§ KOMORAN í˜•íƒœì†Œ ë¶„ì„ì˜ ì¥ì 
        
        - **ì •í™•í•œ í’ˆì‚¬ ë¶„ì„**: ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬ ë“± ì •í™•í•œ ë¶„ë¥˜
        - **ì˜ë¯¸ ìˆëŠ” í‚¤ì›Œë“œ ì¶”ì¶œ**: ë¶ˆìš©ì–´ ì œê±°ë¡œ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ
        - **í•œêµ­ì–´ íŠ¹í™”**: í•œêµ­ì–´ ë¬¸ë²•ì— ìµœì í™”ëœ ë¶„ì„
        """)

def korean_analysis_interface():
    """í•œêµ­ì–´ ë°ì´í„° ë¶„ì„ ì¸í„°í˜ì´ìŠ¤"""
    st.header("ğŸ‡°ğŸ‡· í•œêµ­ì–´ ë°ì´í„° ë¶„ì„")
    st.markdown("ë„¤ì´ë²„ ë‰´ìŠ¤, ë¸”ë¡œê·¸ ë“± í•œêµ­ì–´ ë°ì´í„°ë¥¼ KOMORAN í˜•íƒœì†Œ ë¶„ì„ê¸°ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.")
    
    # ì „ì—­ ìƒíƒœì—ì„œ í‚¤ì›Œë“œ í™•ì¸
    if st.session_state.get('analysis_completed', False):
        korean_keywords = st.session_state.get('korean_keyword_list', [])
        period_days = st.session_state.get('period_days', 90)
        use_naver_news = st.session_state.get('use_naver_news', True)
        use_naver_blog = st.session_state.get('use_naver_blog', True)
        use_naver_datalab = st.session_state.get('use_naver_datalab', True)
        use_morphology = st.session_state.get('use_morphology', True)
        
        # í•œêµ­ì–´ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
        if not korean_keywords:
            st.warning("âš ï¸ í•œêµ­ì–´ í‚¤ì›Œë“œê°€ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í•œêµ­ì–´ê¶Œ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” í•œêµ­ì–´ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        # ì„¤ì • ì •ë³´ í‘œì‹œ
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("í•œêµ­ì–´ í‚¤ì›Œë“œ", ', '.join(korean_keywords))
        with col2:
            st.metric("ë¶„ì„ ê¸°ê°„", f"{period_days}ì¼")
        with col3:
            sources = []
            if use_naver_news:
                sources.append("ë„¤ì´ë²„ ë‰´ìŠ¤")
            if use_naver_blog:
                sources.append("ë„¤ì´ë²„ ë¸”ë¡œê·¸")
            if use_naver_datalab:
                sources.append("ë„¤ì´ë²„ ë°ì´í„°ë©")
            if use_web_search:
                sources.append("ì›¹ ê²€ìƒ‰")
            st.metric("ë°ì´í„° ì†ŒìŠ¤", ', '.join(sources) if sources else "ì—†ìŒ")
        
        # í˜•íƒœì†Œ ë¶„ì„ ì„¤ì • í‘œì‹œ
        if use_morphology:
            st.success("âœ… KOMORAN í˜•íƒœì†Œ ë¶„ì„ í™œì„±í™”")
        else:
            st.info("â„¹ï¸ ê¸°ë³¸ í† í°í™” ì‚¬ìš©")
        
        # ìë™ ë¶„ì„ ì‹œì‘
        with st.spinner("í•œêµ­ì–´ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                # íŠ¸ë Œë“œ ë¶„ì„ê¸° ì´ˆê¸°í™”
                analyzer = TrendAnalyzer()
                
                # í•œêµ­ì–´ ë°ì´í„° ìˆ˜ì§‘ (ê°œì„ ëœ ìˆ˜ì§‘ëŸ‰)
                st.info("ğŸ“Š í•œêµ­ì–´ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
                data = analyzer.collect_korean_data(korean_keywords, period_days, use_naver_news, use_naver_blog, use_naver_datalab, use_web_search)
                
                # ìˆ˜ì§‘ëœ ë°ì´í„° í’ˆì§ˆ í™•ì¸
                if data:
                    news_count = len(data.get('news_data', {}).get('naver_news', []))
                    blog_count = len(data.get('news_data', {}).get('naver_blog', []))
                    web_news_count = len(data.get('news_data', {}).get('web_news', []))
                    web_blog_count = len(data.get('news_data', {}).get('web_blog', []))
                    total_count = news_count + blog_count + web_news_count + web_blog_count
                    st.success(f"âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: ì´ {total_count}ê°œ (ë„¤ì´ë²„ ë‰´ìŠ¤ {news_count}ê°œ, ë„¤ì´ë²„ ë¸”ë¡œê·¸ {blog_count}ê°œ, ì›¹ ë‰´ìŠ¤ {web_news_count}ê°œ, ì›¹ ë¸”ë¡œê·¸ {web_blog_count}ê°œ)")
            else:
                    st.warning("âš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
                # ë¶„ì„ ìˆ˜í–‰ (í˜•íƒœì†Œ ë¶„ì„ í¬í•¨)
                results = analyzer.analyze_korean_data(data, use_morphology)
                
                # ê²°ê³¼ í‘œì‹œ
                display_korean_results(results, korean_keywords)
                
            except Exception as e:
                st.error(f"âŒ í•œêµ­ì–´ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                if Config.DEBUG:
                    st.exception(e)

def display_korean_results(results: dict, keywords: list):
    """í•œêµ­ì–´ ë¶„ì„ ê²°ê³¼ í‘œì‹œ"""
    st.header("ğŸ“Š í•œêµ­ì–´ ë¶„ì„ ê²°ê³¼")
    
    # ë°ì´í„° ìš”ì•½
    st.subheader("ğŸ“ˆ ë°ì´í„° ìš”ì•½")
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
        st.metric("ë¶„ì„ í‚¤ì›Œë“œ", ', '.join(keywords))
                    with col2:
        st.metric("ë¶„ì„ ì‹œì ", results.get('analysis_timestamp', 'N/A')[:10])
                    with col3:
        # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ìš”ì•½
        clustering_results = results.get('clustering_results', {})
        if isinstance(clustering_results, dict) and 'clusters' in clustering_results:
            cluster_count = len(clustering_results['clusters'])
            st.metric("í´ëŸ¬ìŠ¤í„° ìˆ˜", cluster_count)
            else:
            st.metric("í´ëŸ¬ìŠ¤í„° ìˆ˜", "N/A")
    
    # ì‹œê°í™” í‘œì‹œ (í•œêµ­ì–´ ë¶„ì„ ìµœì í™”)
    if 'visualizations' in results:
        visualizations = results['visualizations']
        
        if visualizations:
            st.markdown("---")
            st.subheader("ğŸ“Š í•œêµ­ì–´ ë¶„ì„ ì‹œê°í™”")
            
            # 3ì—´ ë ˆì´ì•„ì›ƒìœ¼ë¡œ ì‹œê°í™” í‘œì‹œ (í•œêµ­ì–´ ë¶„ì„ì— ìµœì í™”)
            col1, col2, col3 = st.columns(3)
            
            with col1:
                # ë‰´ìŠ¤ ì£¼ì œ ì°¨íŠ¸
            if 'news_topics_chart' in visualizations:
                    st.subheader("ğŸ“° ë‰´ìŠ¤ ì£¼ì œ Top 10")
                st.plotly_chart(visualizations['news_topics_chart'], use_container_width=True)
                
                # ë‰´ìŠ¤ ê±´ìˆ˜ ì¶”ì´
                if 'news_count' in visualizations:
                    st.subheader("ğŸ“ˆ ë‰´ìŠ¤ ê±´ìˆ˜ ì¶”ì´")
                    st.plotly_chart(visualizations['news_count'], use_container_width=True)
            
            with col2:
                # ë¸”ë¡œê·¸ ì£¼ì œ ì°¨íŠ¸
                if 'blog_topics_chart' in visualizations:
                    st.subheader("ğŸ“ ë¸”ë¡œê·¸ ì£¼ì œ Top 10")
                    st.plotly_chart(visualizations['blog_topics_chart'], use_container_width=True)
                
                # ë¸”ë¡œê·¸ ê±´ìˆ˜ ì¶”ì´
                if 'blog_count' in visualizations:
                    st.subheader("ğŸ“ˆ ë¸”ë¡œê·¸ ê±´ìˆ˜ ì¶”ì´")
                    st.plotly_chart(visualizations['blog_count'], use_container_width=True)
            
            with col3:
        # ì›Œë“œí´ë¼ìš°ë“œ
        if 'wordcloud' in visualizations:
                    st.subheader("â˜ï¸ í‚¤ì›Œë“œ ì›Œë“œí´ë¼ìš°ë“œ")
                    st.image(visualizations['wordcloud'], caption="KOMORAN ë¶„ì„ ê¸°ë°˜ ì£¼ìš” í‚¤ì›Œë“œ")
                
                # ê°ì„± ë¶„ì„ ì°¨íŠ¸
                if 'sentiment_chart' in visualizations:
                    st.subheader("ğŸ˜Š ê°ì„± ë¶„ì„")
                    st.plotly_chart(visualizations['sentiment_chart'], use_container_width=True)
        else:
            st.info("ì‹œê°í™” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # Raw ë°ì´í„° ì ‘ê¸°/í¼ì¹˜ê¸°
    st.markdown("---")
    with st.expander("ğŸ” Raw ë°ì´í„° ë³´ê¸° (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)", expanded=False):
        st.subheader("ğŸ“Š ìˆ˜ì§‘ëœ ë°ì´í„°")
        
        # ë‰´ìŠ¤ ë°ì´í„°
        news_data = results.get('news_data', {}).get('naver_news', [])
        if news_data:
            st.subheader("ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ ë°ì´í„°")
            news_df = pd.DataFrame(news_data)
            st.dataframe(news_df, use_container_width=True)
        
        # ë¸”ë¡œê·¸ ë°ì´í„°
        blog_data = results.get('news_data', {}).get('naver_blog', [])
        if blog_data:
            st.subheader("ğŸ“ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë°ì´í„°")
            blog_df = pd.DataFrame(blog_data)
            st.dataframe(blog_df, use_container_width=True)
        
        # ì›¹ ë‰´ìŠ¤ ë°ì´í„°
        web_news_data = results.get('news_data', {}).get('web_news', [])
        if web_news_data:
            st.subheader("ğŸŒ ì›¹ ë‰´ìŠ¤ ë°ì´í„°")
            web_news_df = pd.DataFrame(web_news_data)
            st.dataframe(web_news_df, use_container_width=True)
        
        # ì›¹ ë¸”ë¡œê·¸ ë°ì´í„°
        web_blog_data = results.get('news_data', {}).get('web_blog', [])
        if web_blog_data:
            st.subheader("ğŸŒ ì›¹ ë¸”ë¡œê·¸ ë°ì´í„°")
            web_blog_df = pd.DataFrame(web_blog_data)
            st.dataframe(web_blog_df, use_container_width=True)
        
        # ë¶„ì„ ê²°ê³¼
        if 'topic_results' in results:
            st.subheader("ğŸ”¤ í† í”½ ë¶„ì„ ê²°ê³¼")
            topic_results = results['topic_results']
            if isinstance(topic_results, dict):
                for key, value in topic_results.items():
                    st.write(f"**{key}:** {value}")
        
        if 'sentiment_results' in results:
            st.subheader("ğŸ˜Š ê°ì„± ë¶„ì„ ê²°ê³¼")
            sentiment_results = results['sentiment_results']
            if isinstance(sentiment_results, dict):
                for key, value in sentiment_results.items():
                    st.write(f"**{key}:** {value}")
        
        if 'clustering_results' in results:
            st.subheader("ğŸ”— í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼")
            clustering_results = results['clustering_results']
            if isinstance(clustering_results, dict):
                st.json(clustering_results)

if __name__ == "__main__":
    main()