"""
íŠ¸ë Œë“œ ë¶„ì„ ì„œë¹„ìŠ¤ ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ - í•œêµ­ì–´ ë¶„ì„ ì „ìš©
"""
import streamlit as st
import pandas as pd
from datetime import datetime, timedelta
import os
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í„°ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.common.config import Config
from src.common.trace import snapshot_df, log_shape
from data_collectors import NaverCollector
from ai import SentimentAnalyzer, TopicExtractor
from ai.clustering_analyzer import ClusteringAnalyzer
from visualization import ChartGenerator, WordCloudGenerator

class TrendAnalyzer:
    """íŠ¸ë Œë“œ ë¶„ì„ ë©”ì¸ í´ë˜ìŠ¤ - í•œêµ­ì–´ ì „ìš©"""
    
    def __init__(self):
        # ì„¤ì • ê²€ì¦
        Config.validate_config()
        
        # ë°ì´í„° ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” (í•œêµ­ì–´ ì „ìš©)
        self.naver = NaverCollector(Config.NAVER_CLIENT_ID, Config.NAVER_CLIENT_SECRET)
        
        # AI ë¶„ì„ê¸° ì´ˆê¸°í™”
        self.sentiment_analyzer = SentimentAnalyzer(Config.HUGGINGFACE_API_KEY)
        self.topic_extractor = TopicExtractor()
        self.clustering_analyzer = ClusteringAnalyzer()
        
        # ì‹œê°í™” ë„êµ¬ ì´ˆê¸°í™”
        self.chart_generator = ChartGenerator()
        self.wordcloud_generator = WordCloudGenerator()
    
    def collect_korean_data(self, keywords: list, period_days: int, use_naver_news: bool = True, 
                           use_naver_blog: bool = True, use_naver_datalab: bool = False) -> dict:
        """
        í•œêµ­ì–´ ë°ì´í„° ìˆ˜ì§‘
        
        Args:
            keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            period_days: ìˆ˜ì§‘ ê¸°ê°„ (ì¼)
            use_naver_news: ë„¤ì´ë²„ ë‰´ìŠ¤ ì‚¬ìš© ì—¬ë¶€
            use_naver_blog: ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì‚¬ìš© ì—¬ë¶€
            use_naver_datalab: ë„¤ì´ë²„ ë°ì´í„°ë© ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            dict: ìˆ˜ì§‘ëœ ë°ì´í„°
        """
        data = {
            'news_data': {},
            'analysis_timestamp': datetime.now().isoformat(),
            'period_days': period_days,
            'keywords': keywords
        }
        
        # ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘
        if use_naver_news:
            try:
                # í‚¤ì›Œë“œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
                query = ' '.join(keywords) if isinstance(keywords, list) else str(keywords)
                
                with st.spinner("ğŸŒ ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘..."):
                    news_data = self.naver.search_news(query, display=100)
                
                if isinstance(news_data, pd.DataFrame) and not news_data.empty:
                    data['news_data']['naver_news'] = news_data.to_dict('records')
                    st.success(f"âœ… ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {len(news_data)}ê°œ")
                elif isinstance(news_data, pd.DataFrame) and news_data.empty:
                    st.warning("âš ï¸ ë„¤ì´ë²„ ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    st.error(f"âŒ ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: ì˜ëª»ëœ ë°ì´í„° íƒ€ì…")
                    
            except Exception as e:
                st.error(f"âŒ ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
        
        # ë„¤ì´ë²„ ë¸”ë¡œê·¸ ìˆ˜ì§‘
        if use_naver_blog:
            try:
                # í‚¤ì›Œë“œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
                query = ' '.join(keywords) if isinstance(keywords, list) else str(keywords)
                
                with st.spinner("ğŸŒ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ìˆ˜ì§‘ ì¤‘..."):
                    blog_data = self.naver.search_blog(query, display=100)
                
                if isinstance(blog_data, pd.DataFrame) and not blog_data.empty:
                    data['news_data']['naver_blog'] = blog_data.to_dict('records')
                    st.success(f"âœ… ë„¤ì´ë²„ ë¸”ë¡œê·¸ ìˆ˜ì§‘ ì™„ë£Œ: {len(blog_data)}ê°œ")
                elif isinstance(blog_data, pd.DataFrame) and blog_data.empty:
                    st.warning("âš ï¸ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    st.error(f"âŒ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ìˆ˜ì§‘ ì‹¤íŒ¨: ì˜ëª»ëœ ë°ì´í„° íƒ€ì…")
                    
            except Exception as e:
                st.error(f"âŒ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
        
        return data
    
    def analyze_korean_data(self, data: dict, use_morphology: bool = True) -> dict:
        """
        í•œêµ­ì–´ ë°ì´í„° ë¶„ì„
        
        Args:
            data: ìˆ˜ì§‘ëœ ë°ì´í„°
            use_morphology: í˜•íƒœì†Œ ë¶„ì„ ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            dict: ë¶„ì„ ê²°ê³¼
        """
        results = {
            'analysis_timestamp': datetime.now().isoformat(),
            'use_morphology': use_morphology
        }
        
        # í† í”½ ë¶„ì„
        try:
            st.info("ğŸ” í† í”½ ë¶„ì„ ì¤‘...")
            if 'news_data' in data and data['news_data']:
                news_data = data['news_data']
                
                # ë‰´ìŠ¤ í† í”½ ë¶„ì„
                if 'naver_news' in news_data and news_data['naver_news']:
                    news_topics = self.topic_extractor.extract_topics_simple(news_data['naver_news'])
                    results['news_topics'] = news_topics
                    st.success(f"âœ… ë‰´ìŠ¤ í† í”½ ë¶„ì„ ì™„ë£Œ: {len(news_topics)}ê°œ")
                
                # ë¸”ë¡œê·¸ í† í”½ ë¶„ì„
                if 'naver_blog' in news_data and news_data['naver_blog']:
                    blog_topics = self.topic_extractor.extract_topics_simple(news_data['naver_blog'])
                    results['blog_topics'] = blog_topics
                    st.success(f"âœ… ë¸”ë¡œê·¸ í† í”½ ë¶„ì„ ì™„ë£Œ: {len(blog_topics)}ê°œ")
        except Exception as e:
            st.error(f"âŒ í† í”½ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        
        # ê°ì„± ë¶„ì„
        try:
            st.info("ğŸ˜Š ê°ì„± ë¶„ì„ ì¤‘...")
            if 'news_data' in data and data['news_data']:
                all_texts = []
                news_data = data['news_data']
                
                if 'naver_news' in news_data and news_data['naver_news']:
                    for news in news_data['naver_news']:
                        if 'text_clean' in news:
                            all_texts.append(news['text_clean'])
                        elif 'description' in news:
                            all_texts.append(news['description'])
                
                if 'naver_blog' in news_data and news_data['naver_blog']:
                    for blog in news_data['naver_blog']:
                        if 'text_clean' in blog:
                            all_texts.append(blog['text_clean'])
                        elif 'description' in blog:
                            all_texts.append(blog['description'])
                
                if all_texts:
                    sentiment_results = self.sentiment_analyzer.analyze_batch_sentiment(all_texts)
                    results['sentiment_results'] = sentiment_results
                    st.success("âœ… ê°ì„± ë¶„ì„ ì™„ë£Œ")
        except Exception as e:
            st.error(f"âŒ ê°ì„± ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        
        # í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„
        try:
            st.info("ğŸ”— í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ì¤‘...")
            if 'news_data' in data and data['news_data']:
                all_texts = []
                news_data = data['news_data']
                
                if 'naver_news' in news_data and news_data['naver_news']:
                    for news in news_data['naver_news']:
                        if 'text_clean' in news:
                            all_texts.append(news['text_clean'])
                        elif 'description' in news:
                            all_texts.append(news['description'])
                
                if 'naver_blog' in news_data and news_data['naver_blog']:
                    for blog in news_data['naver_blog']:
                        if 'text_clean' in blog:
                            all_texts.append(blog['text_clean'])
                        elif 'description' in blog:
                            all_texts.append(blog['description'])
                
                if all_texts:
                    clustering_results = self.clustering_analyzer.cluster_documents(all_texts)
                    results['clustering_results'] = clustering_results
                    st.success("âœ… í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ì™„ë£Œ")
        except Exception as e:
            st.error(f"âŒ í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        
        # ì‹œê°í™” ìƒì„±
        try:
            st.info("ğŸ“Š ì‹œê°í™” ìƒì„± ì¤‘...")
            visualizations = self._create_visualizations(data, results)
            results['visualizations'] = visualizations
            st.success("âœ… ì‹œê°í™” ìƒì„± ì™„ë£Œ")
        except Exception as e:
            st.error(f"âŒ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {str(e)}")
        
        return results
    
    def _create_visualizations(self, data: dict, results: dict) -> dict:
        """ì‹œê°í™” ìƒì„±"""
        visualizations = {}
        
        try:
            # ë‰´ìŠ¤ ì£¼ì œ ì°¨íŠ¸
            if 'news_topics' in results and results['news_topics']:
                visualizations['news_topics_chart'] = self.chart_generator.create_topic_frequency_chart(results['news_topics'])
            
            # ë¸”ë¡œê·¸ ì£¼ì œ ì°¨íŠ¸
            if 'blog_topics' in results and results['blog_topics']:
                visualizations['blog_topics_chart'] = self.chart_generator.create_topic_frequency_chart(results['blog_topics'])
            
            # ë‰´ìŠ¤ ê±´ìˆ˜ ì¶”ì´
            if 'news_data' in data and data['news_data'] and 'naver_news' in data['news_data']:
                news_df = pd.DataFrame(data['news_data']['naver_news'])
                if not news_df.empty and 'pub_date' in news_df.columns:
                    news_df['date'] = pd.to_datetime(news_df['pub_date'], errors='coerce')
                    news_df = news_df.dropna(subset=['date'])
                    if not news_df.empty:
                        visualizations['news_count'] = self.chart_generator.create_news_count_chart({
                            'naver_news': news_df
                        })
            
            # ë¸”ë¡œê·¸ ê±´ìˆ˜ ì¶”ì´
            if 'news_data' in data and data['news_data'] and 'naver_blog' in data['news_data']:
                blog_df = pd.DataFrame(data['news_data']['naver_blog'])
                if not blog_df.empty and 'pub_date' in blog_df.columns:
                    blog_df['date'] = pd.to_datetime(blog_df['pub_date'], errors='coerce')
                    blog_df = blog_df.dropna(subset=['date'])
                    if not blog_df.empty:
                        visualizations['blog_count'] = self.chart_generator.create_news_count_chart({
                            'naver_blog': blog_df
                        })
            
            # ì›Œë“œí´ë¼ìš°ë“œ
            if 'news_data' in data and data['news_data']:
                all_texts = []
                news_data = data['news_data']
                
                if 'naver_news' in news_data and news_data['naver_news']:
                    for news in news_data['naver_news']:
                        if 'text_clean' in news:
                            all_texts.append(news['text_clean'])
                        elif 'description' in news:
                            all_texts.append(news['description'])
                
                if 'naver_blog' in news_data and news_data['naver_blog']:
                    for blog in news_data['naver_blog']:
                        if 'text_clean' in blog:
                            all_texts.append(blog['text_clean'])
                        elif 'description' in blog:
                            all_texts.append(blog['description'])
                
                if all_texts:
                    topic_results = self.topic_extractor.extract_topics_simple(all_texts)
                    if topic_results:
                        word_freq = {t['word']: t['count'] for t in topic_results}
                        visualizations['wordcloud'] = self.wordcloud_generator.generate_from_frequency(word_freq)
            
            # ê°ì„± ë¶„ì„ ì°¨íŠ¸
            if 'sentiment_results' in results and results['sentiment_results']:
                visualizations['sentiment_chart'] = self.chart_generator.create_sentiment_chart(results['sentiment_results'])
        
        except Exception as e:
            st.warning(f"âš ï¸ ì‹œê°í™” ìƒì„± ì¤‘ ì¼ë¶€ ì˜¤ë¥˜: {str(e)}")
        
        return visualizations

def main():
    """ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ - í•œêµ­ì–´ ë¶„ì„ ì „ìš©"""
    st.set_page_config(
        page_title="Trendbot - íŠ¸ë Œë“œ ë¶„ì„",
        page_icon="ğŸ“Š",
        layout="wide"
    )
    
    st.title("ğŸ“Š Trendbot - íŠ¸ë Œë“œ ë¶„ì„")
    st.markdown("**í•œêµ­ì–´ í‚¤ì›Œë“œ**ë¡œ ë‰´ìŠ¤, ë¸”ë¡œê·¸ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  **KOMORAN í˜•íƒœì†Œ ë¶„ì„**ìœ¼ë¡œ ì •í™•í•œ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.")
    
    # ì‚¬ì´ë“œë°”ì— ê²€ìƒ‰ì°½ê³¼ ì„¤ì •
    with st.sidebar:
        st.header("ğŸ” ë¶„ì„ ì„¤ì •")
        
        # í•œêµ­ì–´ í‚¤ì›Œë“œ ì…ë ¥
        st.subheader("ğŸ”¤ í•œêµ­ì–´ í‚¤ì›Œë“œ")
        korean_keywords = st.text_input(
            "ğŸ‡°ğŸ‡· ë¶„ì„í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”",
            placeholder="ì˜ˆ: ìƒì„±í˜• ai, ì¸ê³µì§€ëŠ¥, ë¨¸ì‹ ëŸ¬ë‹ (ì‰¼í‘œë¡œ êµ¬ë¶„)",
            key="korean_keywords"
        )
        
        if not korean_keywords:
            st.warning("âš ï¸ í•œêµ­ì–´ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            analyze_button = False
        else:
            analyze_button = True
        
        # ìˆ˜ì§‘ ê¸°ê°„ ì„¤ì •
        period_days = st.selectbox(
            "ë¶„ì„ ê¸°ê°„",
            options=[30, 90, 180, 365, 730, 1095, 1460, 1825],
            index=1,  # ê¸°ë³¸ê°’: 90ì¼
            format_func=lambda x: f"{x}ì¼ ({x//365}ë…„ {x%365//30}ê°œì›”)" if x >= 365 else f"{x}ì¼",
            key="main_period_days"
        )
        
        # ë°ì´í„° ì†ŒìŠ¤ ì„ íƒ
        st.subheader("ğŸ“Š í•œêµ­ì–´ ë°ì´í„° ì†ŒìŠ¤")
        use_naver_news = st.checkbox("ë„¤ì´ë²„ ë‰´ìŠ¤", value=True, key="sidebar_naver_news")
        use_naver_blog = st.checkbox("ë„¤ì´ë²„ ë¸”ë¡œê·¸", value=True, key="sidebar_naver_blog")
        use_naver_datalab = st.checkbox("ë„¤ì´ë²„ ë°ì´í„°ë©", value=False, key="sidebar_naver_datalab")
        
        # í˜•íƒœì†Œ ë¶„ì„ ì„¤ì •
        st.subheader("ğŸ”¤ KOMORAN ë¶„ì„")
        use_morphology = st.checkbox("KOMORAN í˜•íƒœì†Œ ë¶„ì„ ì‚¬ìš©", value=True, help="ì •í™•í•œ í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„", key="sidebar_morphology")
        if use_morphology:
            st.info("âœ… ì •í™•í•œ í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.")
        
        # ë¶„ì„ ì‹œì‘ ë²„íŠ¼
        analyze_button = st.button("ğŸš€ íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘", type="primary", key="main_analyze", use_container_width=True)
        
        if analyze_button and korean_keywords:
            # í‚¤ì›Œë“œ ë¶„ë¦¬ ë° ì²˜ë¦¬
            korean_keyword_list = [kw.strip() for kw in korean_keywords.split(',') if kw.strip()]
            
            # ì „ì—­ ìƒíƒœì— í‚¤ì›Œë“œì™€ ì„¤ì • ì €ì¥
            st.session_state['korean_keyword_list'] = korean_keyword_list
            st.session_state['period_days'] = period_days
            st.session_state['use_naver_news'] = use_naver_news
            st.session_state['use_naver_blog'] = use_naver_blog
            st.session_state['use_naver_datalab'] = use_naver_datalab
            st.session_state['use_morphology'] = use_morphology
            st.session_state['analysis_completed'] = True
            
            st.success("âœ… íŠ¸ë Œë“œ ë¶„ì„ ì„¤ì •ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    # í‚¤ì›Œë“œ í‘œì‹œ (ë©”ì¸ ì˜ì—­)
    if st.session_state.get('analysis_completed', False):
        st.header("ğŸ“ ë¶„ì„ í‚¤ì›Œë“œ")
        
        korean_keywords = st.session_state.get('korean_keyword_list', [])
        if korean_keywords:
            st.write("**ğŸ‡°ğŸ‡· í•œêµ­ì–´ í‚¤ì›Œë“œ:**")
            for keyword in korean_keywords:
                st.write(f"- {keyword}")
        
        # í•œêµ­ì–´ ë¶„ì„ ì‹¤í–‰
        korean_analysis_interface()
    else:
        # ì´ˆê¸° í™”ë©´
        st.markdown("""
        ## ğŸ¯ íŠ¸ë Œë“œ ë¶„ì„ ì‚¬ìš©ë²•
        
        1. **ì™¼ìª½ ì‚¬ì´ë“œë°”**ì—ì„œ ë¶„ì„í•  **í•œêµ­ì–´ í‚¤ì›Œë“œ**ë¥¼ ì…ë ¥í•˜ì„¸ìš”
        2. **ë¶„ì„ ê¸°ê°„**ì„ ì„ íƒí•˜ì„¸ìš” (ìµœëŒ€ 5ë…„)
        3. **ë°ì´í„° ì†ŒìŠ¤**ë¥¼ ì„ íƒí•˜ì„¸ìš” (Naver ë‰´ìŠ¤/ë¸”ë¡œê·¸)
        4. **íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘** ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”
        
        ### ğŸ“Š ì œê³µë˜ëŠ” ë¶„ì„ ê¸°ëŠ¥
        
        - **ë‰´ìŠ¤ ë¶„ì„**: Naver ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„
        - **ë¸”ë¡œê·¸ ë¶„ì„**: Naver ë¸”ë¡œê·¸ ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„
        - **KOMORAN í˜•íƒœì†Œ ë¶„ì„**: ì •í™•í•œ í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë¶„ì„
        - **ê°ì„± ë¶„ì„**: í•œêµ­ì–´ í…ìŠ¤íŠ¸ì˜ ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ë¶„ì„
        - **í† í”½ ë¶„ì„**: ì£¼ìš” í‚¤ì›Œë“œì™€ ì£¼ì œ ì¶”ì¶œ
        - **í´ëŸ¬ìŠ¤í„°ë§**: ìœ ì‚¬í•œ ë‚´ìš© ê·¸ë£¹í™”
        - **ì‹œê°í™”**: ì°¨íŠ¸ì™€ ì›Œë“œí´ë¼ìš°ë“œ
        
        ### ğŸ”§ KOMORAN í˜•íƒœì†Œ ë¶„ì„ì˜ ì¥ì 
        
        - **ì •í™•í•œ í’ˆì‚¬ ë¶„ì„**: ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬ ë“± ì •í™•í•œ ë¶„ë¥˜
        - **ì˜ë¯¸ ìˆëŠ” í‚¤ì›Œë“œ ì¶”ì¶œ**: ë¶ˆìš©ì–´ ì œê±°ë¡œ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ
        - **í•œêµ­ì–´ íŠ¹í™”**: í•œêµ­ì–´ ë¬¸ë²•ì— ìµœì í™”ëœ ë¶„ì„
        """)

def korean_analysis_interface():
    """í•œêµ­ì–´ ë°ì´í„° ë¶„ì„ ì¸í„°í˜ì´ìŠ¤"""
    st.header("ğŸ‡°ğŸ‡· í•œêµ­ì–´ ë°ì´í„° ë¶„ì„")
    st.markdown("ë„¤ì´ë²„ ë‰´ìŠ¤, ë¸”ë¡œê·¸ ë“± í•œêµ­ì–´ ë°ì´í„°ë¥¼ KOMORAN í˜•íƒœì†Œ ë¶„ì„ê¸°ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.")
    
    # ì „ì—­ ìƒíƒœì—ì„œ í‚¤ì›Œë“œ í™•ì¸
    if st.session_state.get('analysis_completed', False):
        korean_keywords = st.session_state.get('korean_keyword_list', [])
        period_days = st.session_state.get('period_days', 90)
        use_naver_news = st.session_state.get('use_naver_news', True)
        use_naver_blog = st.session_state.get('use_naver_blog', True)
        use_naver_datalab = st.session_state.get('use_naver_datalab', True)
        use_morphology = st.session_state.get('use_morphology', True)
        
        # í•œêµ­ì–´ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
        if not korean_keywords:
            st.warning("âš ï¸ í•œêµ­ì–´ í‚¤ì›Œë“œê°€ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í•œêµ­ì–´ê¶Œ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” í•œêµ­ì–´ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        # ì„¤ì • ì •ë³´ í‘œì‹œ
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("í•œêµ­ì–´ í‚¤ì›Œë“œ", ', '.join(korean_keywords))
        with col2:
            st.metric("ë¶„ì„ ê¸°ê°„", f"{period_days}ì¼")
        with col3:
            sources = []
            if use_naver_news:
                sources.append("ë„¤ì´ë²„ ë‰´ìŠ¤")
            if use_naver_blog:
                sources.append("ë„¤ì´ë²„ ë¸”ë¡œê·¸")
            if use_naver_datalab:
                sources.append("ë„¤ì´ë²„ ë°ì´í„°ë©")
            st.metric("ë°ì´í„° ì†ŒìŠ¤", ', '.join(sources) if sources else "ì—†ìŒ")
        
        # í˜•íƒœì†Œ ë¶„ì„ ì„¤ì • í‘œì‹œ
        if use_morphology:
            st.success("âœ… KOMORAN í˜•íƒœì†Œ ë¶„ì„ í™œì„±í™”")
        else:
            st.info("â„¹ï¸ ê¸°ë³¸ í† í°í™” ì‚¬ìš©")
        
        # ìë™ ë¶„ì„ ì‹œì‘
        with st.spinner("í•œêµ­ì–´ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                # íŠ¸ë Œë“œ ë¶„ì„ê¸° ì´ˆê¸°í™”
                analyzer = TrendAnalyzer()
                
                # í•œêµ­ì–´ ë°ì´í„° ìˆ˜ì§‘ (ê°œì„ ëœ ìˆ˜ì§‘ëŸ‰)
                st.info("ğŸ“Š í•œêµ­ì–´ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
                data = analyzer.collect_korean_data(korean_keywords, period_days, use_naver_news, use_naver_blog, use_naver_datalab)
                
                # ìˆ˜ì§‘ëœ ë°ì´í„° í’ˆì§ˆ í™•ì¸
                if data:
                    news_count = len(data.get('news_data', {}).get('naver_news', []))
                    blog_count = len(data.get('news_data', {}).get('naver_blog', []))
                    st.success(f"âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: ë‰´ìŠ¤ {news_count}ê°œ, ë¸”ë¡œê·¸ {blog_count}ê°œ")
                else:
                    st.warning("âš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
                # ë¶„ì„ ìˆ˜í–‰ (í˜•íƒœì†Œ ë¶„ì„ í¬í•¨)
                results = analyzer.analyze_korean_data(data, use_morphology)
                
                # ê²°ê³¼ í‘œì‹œ
                display_korean_results(results, korean_keywords)
                
            except Exception as e:
                st.error(f"âŒ í•œêµ­ì–´ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                if Config.DEBUG:
                    st.exception(e)

def display_korean_results(results: dict, keywords: list):
    """í•œêµ­ì–´ ë¶„ì„ ê²°ê³¼ í‘œì‹œ"""
    st.header("ğŸ“Š í•œêµ­ì–´ ë¶„ì„ ê²°ê³¼")
    
    # ë°ì´í„° ìš”ì•½
    st.subheader("ğŸ“ˆ ë°ì´í„° ìš”ì•½")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("ë¶„ì„ í‚¤ì›Œë“œ", ', '.join(keywords))
    with col2:
        st.metric("ë¶„ì„ ì‹œì ", results.get('analysis_timestamp', 'N/A')[:10])
    with col3:
        # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ìš”ì•½
        clustering_results = results.get('clustering_results', {})
        if isinstance(clustering_results, dict) and 'clusters' in clustering_results:
            cluster_count = len(clustering_results['clusters'])
            st.metric("í´ëŸ¬ìŠ¤í„° ìˆ˜", cluster_count)
        else:
            st.metric("í´ëŸ¬ìŠ¤í„° ìˆ˜", "N/A")
    
    # ì‹œê°í™” í‘œì‹œ (í•œêµ­ì–´ ë¶„ì„ ìµœì í™”)
    if 'visualizations' in results:
        visualizations = results['visualizations']
        
        if visualizations:
            st.markdown("---")
            st.subheader("ğŸ“Š í•œêµ­ì–´ ë¶„ì„ ì‹œê°í™”")
            
            # 3ì—´ ë ˆì´ì•„ì›ƒìœ¼ë¡œ ì‹œê°í™” í‘œì‹œ (í•œêµ­ì–´ ë¶„ì„ì— ìµœì í™”)
            col1, col2, col3 = st.columns(3)
            
            with col1:
                # ë‰´ìŠ¤ ì£¼ì œ ì°¨íŠ¸
                if 'news_topics_chart' in visualizations:
                    st.subheader("ğŸ“° ë‰´ìŠ¤ ì£¼ì œ Top 10")
                    st.plotly_chart(visualizations['news_topics_chart'], use_container_width=True)
                
                # ë‰´ìŠ¤ ê±´ìˆ˜ ì¶”ì´
                if 'news_count' in visualizations:
                    st.subheader("ğŸ“ˆ ë‰´ìŠ¤ ê±´ìˆ˜ ì¶”ì´")
                    st.plotly_chart(visualizations['news_count'], use_container_width=True)
            
            with col2:
                # ë¸”ë¡œê·¸ ì£¼ì œ ì°¨íŠ¸
                if 'blog_topics_chart' in visualizations:
                    st.subheader("ğŸ“ ë¸”ë¡œê·¸ ì£¼ì œ Top 10")
                    st.plotly_chart(visualizations['blog_topics_chart'], use_container_width=True)
                
                # ë¸”ë¡œê·¸ ê±´ìˆ˜ ì¶”ì´
                if 'blog_count' in visualizations:
                    st.subheader("ğŸ“ˆ ë¸”ë¡œê·¸ ê±´ìˆ˜ ì¶”ì´")
                    st.plotly_chart(visualizations['blog_count'], use_container_width=True)
            
            with col3:
                # ì›Œë“œí´ë¼ìš°ë“œ
                if 'wordcloud' in visualizations:
                    st.subheader("â˜ï¸ í‚¤ì›Œë“œ ì›Œë“œí´ë¼ìš°ë“œ")
                    st.image(visualizations['wordcloud'], caption="KOMORAN ë¶„ì„ ê¸°ë°˜ ì£¼ìš” í‚¤ì›Œë“œ")
                
                # ê°ì„± ë¶„ì„ ì°¨íŠ¸
                if 'sentiment_chart' in visualizations:
                    st.subheader("ğŸ˜Š ê°ì„± ë¶„ì„")
                    st.plotly_chart(visualizations['sentiment_chart'], use_container_width=True)
        else:
            st.info("ì‹œê°í™” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # Raw ë°ì´í„° ì ‘ê¸°/í¼ì¹˜ê¸°
    st.markdown("---")
    with st.expander("ğŸ” Raw ë°ì´í„° ë³´ê¸° (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)", expanded=False):
        st.subheader("ğŸ“Š ìˆ˜ì§‘ëœ ë°ì´í„°")
        
        # ë‰´ìŠ¤ ë°ì´í„°
        news_data = results.get('news_data', {}).get('naver_news', [])
        if news_data:
            st.subheader("ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ ë°ì´í„°")
            news_df = pd.DataFrame(news_data)
            st.dataframe(news_df, use_container_width=True)
        
        # ë¸”ë¡œê·¸ ë°ì´í„°
        blog_data = results.get('news_data', {}).get('naver_blog', [])
        if blog_data:
            st.subheader("ğŸ“ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë°ì´í„°")
            blog_df = pd.DataFrame(blog_data)
            st.dataframe(blog_df, use_container_width=True)
        
        # ë¶„ì„ ê²°ê³¼
        if 'topic_results' in results:
            st.subheader("ğŸ”¤ í† í”½ ë¶„ì„ ê²°ê³¼")
            topic_results = results['topic_results']
            if isinstance(topic_results, dict):
                for key, value in topic_results.items():
                    st.write(f"**{key}:** {value}")
        
        if 'sentiment_results' in results:
            st.subheader("ğŸ˜Š ê°ì„± ë¶„ì„ ê²°ê³¼")
            sentiment_results = results['sentiment_results']
            if isinstance(sentiment_results, dict):
                for key, value in sentiment_results.items():
                    st.write(f"**{key}:** {value}")
        
        if 'clustering_results' in results:
            st.subheader("ğŸ”— í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼")
            clustering_results = results['clustering_results']
            if isinstance(clustering_results, dict):
                st.json(clustering_results)

if __name__ == "__main__":
    main()