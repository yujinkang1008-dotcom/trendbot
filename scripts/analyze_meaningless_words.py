"""
ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´ë“¤ì´ ì–´ë””ì„œ ì˜¤ëŠ”ì§€ ë¶„ì„í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""
import os
import sys
from datetime import datetime, timedelta
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv("trendbot.env")

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def analyze_raw_data_sources():
    """ì›ë³¸ ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´ ë¶„ì„"""
    print("=" * 60)
    print("ğŸ” ì›ë³¸ ë°ì´í„° ì†ŒìŠ¤ ë¶„ì„")
    print("=" * 60)
    
    try:
        from src.data_collectors.naver_collector import search_news
        from src.data_collectors.arxiv_collector import ArxivCollector
        
        # Naver ë‰´ìŠ¤ ì›ë³¸ ë°ì´í„° ë¶„ì„
        print("\nğŸ“° Naver ë‰´ìŠ¤ ì›ë³¸ ë°ì´í„° ë¶„ì„:")
        df = search_news("ì¸ê³µì§€ëŠ¥", display=3)
        
        for i, (_, row) in enumerate(df.iterrows()):
            title = row.get('title', '')
            desc = row.get('desc', '')
            
            print(f"\n  ğŸ“„ ë‰´ìŠ¤ {i+1}:")
            print(f"    ì œëª©: {title}")
            print(f"    ì„¤ëª…: {desc}")
            
            # HTML íƒœê·¸ë‚˜ íŠ¹ìˆ˜ ë¬¸ì ë¶„ì„
            if '<' in title or '<' in desc:
                print(f"    âš ï¸ HTML íƒœê·¸ ë°œê²¬")
            if '&' in title or '&' in desc:
                print(f"    âš ï¸ HTML ì—”í‹°í‹° ë°œê²¬")
            if 'http' in title or 'http' in desc:
                print(f"    âš ï¸ URL ë°œê²¬")
        
        # arXiv ë…¼ë¬¸ ì›ë³¸ ë°ì´í„° ë¶„ì„
        print("\nğŸ“š arXiv ë…¼ë¬¸ ì›ë³¸ ë°ì´í„° ë¶„ì„:")
        collector = ArxivCollector()
        df_papers = collector.collect_papers(["artificial intelligence"], max_results=2)
        
        for i, (_, row) in enumerate(df_papers.iterrows()):
            title = row.get('title', '')
            summary = row.get('summary', '')
            
            print(f"\n  ğŸ“„ ë…¼ë¬¸ {i+1}:")
            print(f"    ì œëª©: {title}")
            print(f"    ìš”ì•½: {summary[:200]}...")
            
            # íŠ¹ìˆ˜ ë¬¸ì ë¶„ì„
            if '<' in title or '<' in summary:
                print(f"    âš ï¸ HTML íƒœê·¸ ë°œê²¬")
            if '&' in title or '&' in summary:
                print(f"    âš ï¸ HTML ì—”í‹°í‹° ë°œê²¬")
            if 'http' in title or 'http' in summary:
                print(f"    âš ï¸ URL ë°œê²¬")
        
    except Exception as e:
        print(f"âŒ ì›ë³¸ ë°ì´í„° ë¶„ì„ ì˜¤ë¥˜: {e}")

def analyze_text_cleaning_process():
    """í…ìŠ¤íŠ¸ ì •ì œ ê³¼ì • ë¶„ì„"""
    print("=" * 60)
    print("ğŸ” í…ìŠ¤íŠ¸ ì •ì œ ê³¼ì • ë¶„ì„")
    print("=" * 60)
    
    try:
        from src.nlp.clean import normalize_for_topics
        import re
        import html
        
        # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ë“¤
        test_texts = [
            "ì¸ê³µì§€ëŠ¥(AI) ê¸°ìˆ ì´ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤",
            "&nbsp;font href=https://example.com AI ê¸°ìˆ ",
            "RSS XML JSON API ê´€ë ¨ ë‚´ìš©ì…ë‹ˆë‹¤",
            "<b>ì¸ê³µì§€ëŠ¥</b> &amp; ë¨¸ì‹ ëŸ¬ë‹ ê¸°ìˆ ",
            "AI ê¸°ìˆ ì˜ ë°œì „ì´ ê°€ì†í™”ë˜ê³  ìˆìŠµë‹ˆë‹¤"
        ]
        
        for i, text in enumerate(test_texts):
            print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ {i+1}:")
            print(f"  ì›ë³¸: {text}")
            
            # ë‹¨ê³„ë³„ ì •ì œ ê³¼ì • ì¶”ì 
            step1 = html.unescape(text)
            print(f"  HTML ë””ì½”ë“œ: {step1}")
            
            step2 = re.sub(r'<[^>]+>', '', step1)
            print(f"  HTML íƒœê·¸ ì œê±°: {step2}")
            
            step3 = re.sub(r'&[a-zA-Z0-9#]+;', '', step2)
            print(f"  HTML ì—”í‹°í‹° ì œê±°: {step3}")
            
            step4 = re.sub(r'https?://[^\s]+', '', step3)
            step4 = re.sub(r'www\.[^\s]+', '', step4)
            print(f"  URL ì œê±°: {step4}")
            
            step5 = re.sub(r'[^\w\sê°€-í£]', ' ', step4)
            print(f"  íŠ¹ìˆ˜ë¬¸ì ì œê±°: {step5}")
            
            final = normalize_for_topics(text)
            print(f"  ìµœì¢… ê²°ê³¼: {final}")
            
            # ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´ ì²´í¬
            meaningless_words = ['rss', 'xml', 'json', 'api', 'http', 'www', 'com']
            found = [word for word in meaningless_words if word in final.lower()]
            if found:
                print(f"  âš ï¸ ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´ ë°œê²¬: {found}")
            else:
                print(f"  âœ… ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´ ì—†ìŒ")
        
    except Exception as e:
        print(f"âŒ í…ìŠ¤íŠ¸ ì •ì œ ë¶„ì„ ì˜¤ë¥˜: {e}")

def analyze_topic_extraction_process():
    """í† í”½ ì¶”ì¶œ ê³¼ì • ë¶„ì„"""
    print("=" * 60)
    print("ğŸ” í† í”½ ì¶”ì¶œ ê³¼ì • ë¶„ì„")
    print("=" * 60)
    
    try:
        from src.ai.topic_extractor import TopicExtractor
        from src.data_collectors.naver_collector import NaverCollector
        from src.data_collectors.arxiv_collector import ArxivCollector
        
        # ì‹¤ì œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
        naver_collector = NaverCollector(os.getenv("NAVER_CLIENT_ID"), os.getenv("NAVER_CLIENT_SECRET"))
        arxiv_collector = ArxivCollector()
        
        # ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘
        news_data = naver_collector.collect_all_news(["ì¸ê³µì§€ëŠ¥"])
        all_texts = []
        
        for source, articles in news_data.items():
            for article in articles[:3]:  # ì²˜ìŒ 3ê°œë§Œ
                text = f"{article.get('title', '')} {article.get('description', '')}"
                if text.strip():
                    all_texts.append(text)
        
        print(f"ğŸ“Š ìˆ˜ì§‘ëœ í…ìŠ¤íŠ¸: {len(all_texts)}ê°œ")
        
        # ê° í…ìŠ¤íŠ¸ì˜ ì›ë³¸ ë‚´ìš© í™•ì¸
        print("\nğŸ“‹ ì›ë³¸ í…ìŠ¤íŠ¸ ìƒ˜í”Œ:")
        for i, text in enumerate(all_texts[:3]):
            print(f"  {i+1}: {text[:100]}...")
            
            # ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” íŒ¨í„´ ì²´í¬
            if 'AI' in text:
                print(f"    âš ï¸ 'AI' ë°œê²¬")
            if '<' in text:
                print(f"    âš ï¸ HTML íƒœê·¸ ë°œê²¬")
            if '&' in text:
                print(f"    âš ï¸ HTML ì—”í‹°í‹° ë°œê²¬")
            if 'http' in text:
                print(f"    âš ï¸ URL ë°œê²¬")
        
        # í† í”½ ì¶”ì¶œ
        print("\nğŸ¯ í† í”½ ì¶”ì¶œ ê²°ê³¼:")
        extractor = TopicExtractor()
        topics = extractor.top_topics(all_texts, k=10)
        
        print(f"ì¶”ì¶œëœ í† í”½: {topics}")
        
        # ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´ ì²´í¬
        meaningless_words = ['ai', 'rss', 'xml', 'json', 'api', 'http', 'www', 'com', 'target', 'oc']
        found = []
        for topic in topics:
            for word in meaningless_words:
                if word in topic.lower():
                    found.append(f"{topic}({word})")
        
        if found:
            print(f"âš ï¸ ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´ ë°œê²¬: {found}")
        else:
            print(f"âœ… ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´ ì—†ìŒ")
        
    except Exception as e:
        print(f"âŒ í† í”½ ì¶”ì¶œ ë¶„ì„ ì˜¤ë¥˜: {e}")

def analyze_tfidf_features():
    """TF-IDF íŠ¹ì„± ë¶„ì„"""
    print("=" * 60)
    print("ğŸ” TF-IDF íŠ¹ì„± ë¶„ì„")
    print("=" * 60)
    
    try:
        from src.ai.topic_extractor import TopicExtractor
        from sklearn.feature_extraction.text import TfidfVectorizer
        import numpy as np
        
        # í…ŒìŠ¤íŠ¸ ì½”í¼ìŠ¤
        test_corpus = [
            "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì´ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤",
            "ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì„ ì—°êµ¬í•©ë‹ˆë‹¤",
            "ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ê°œë°œí•©ë‹ˆë‹¤",
            "AI ê¸°ìˆ ì˜ ë°œì „ì´ ê°€ì†í™”ë˜ê³  ìˆìŠµë‹ˆë‹¤",  # ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸
            "RSS XML JSON API ê´€ë ¨ ë‚´ìš©ì…ë‹ˆë‹¤"  # ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸
        ]
        
        print("ğŸ“Š í…ŒìŠ¤íŠ¸ ì½”í¼ìŠ¤:")
        for i, text in enumerate(test_corpus):
            print(f"  {i+1}: {text}")
        
        # TF-IDF ë²¡í„°ë¼ì´ì € ì„¤ì • (í† í”½ ì¶”ì¶œê¸°ì™€ ë™ì¼í•œ ì„¤ì •)
        from src.nlp.clean import STOPWORDS, GARBAGE_TOKENS
        combined_stopwords = STOPWORDS | GARBAGE_TOKENS
        
        vectorizer = TfidfVectorizer(
            ngram_range=(1, 2),
            min_df=1,
            max_df=0.8,
            stop_words=list(combined_stopwords),
            token_pattern=r'(?u)\b[ê°€-í£a-zA-Z]{2,}\b'
        )
        
        # í…ìŠ¤íŠ¸ ì •ì œ
        extractor = TopicExtractor()
        cleaned_corpus = []
        for text in test_corpus:
            clean_text = extractor.preprocess_text(text)
            cleaned_corpus.append(clean_text)
        
        print("\nğŸ§¹ ì •ì œëœ ì½”í¼ìŠ¤:")
        for i, text in enumerate(cleaned_corpus):
            print(f"  {i+1}: {text}")
        
        # TF-IDF í–‰ë ¬ ìƒì„±
        tfidf_matrix = vectorizer.fit_transform(cleaned_corpus)
        feature_names = vectorizer.get_feature_names_out()
        
        print(f"\nğŸ“ˆ TF-IDF íŠ¹ì„±:")
        print(f"  íŠ¹ì„± ìˆ˜: {len(feature_names)}")
        print(f"  íŠ¹ì„± ìƒ˜í”Œ: {feature_names[:20]}")
        
        # í‰ê·  TF-IDF ì ìˆ˜ ê³„ì‚°
        mean_scores = np.mean(tfidf_matrix.toarray(), axis=0)
        
        # ìƒìœ„ íŠ¹ì„±ë“¤
        top_indices = np.argsort(mean_scores)[::-1][:10]
        top_features = [(feature_names[i], mean_scores[i]) for i in top_indices]
        
        print(f"\nğŸ¯ ìƒìœ„ íŠ¹ì„±ë“¤:")
        for feature, score in top_features:
            print(f"  {feature}: {score:.4f}")
            
            # ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´ ì²´í¬
            meaningless_words = ['ai', 'rss', 'xml', 'json', 'api', 'http', 'www', 'com']
            if any(word in feature.lower() for word in meaningless_words):
                print(f"    âš ï¸ ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´ í¬í•¨!")
        
    except Exception as e:
        print(f"âŒ TF-IDF ë¶„ì„ ì˜¤ë¥˜: {e}")

def main():
    """ë©”ì¸ ë¶„ì„ í•¨ìˆ˜"""
    print("ğŸš€ ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´ ë¶„ì„ ì‹œì‘")
    print("=" * 60)
    
    analyze_raw_data_sources()
    analyze_text_cleaning_process()
    analyze_topic_extraction_process()
    analyze_tfidf_features()
    
    print("\n" + "=" * 60)
    print("âœ… ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´ ë¶„ì„ ì™„ë£Œ")
    print("=" * 60)

if __name__ == "__main__":
    main()
