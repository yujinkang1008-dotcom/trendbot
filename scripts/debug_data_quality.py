"""
ë°ì´í„° í’ˆì§ˆ ë””ë²„ê¹… ìŠ¤í¬ë¦½íŠ¸
ê° ë°ì´í„° ì†ŒìŠ¤ë³„ë¡œ í’ˆì§ˆì„ ê²€ì¦í•˜ê³  ë¬¸ì œì ì„ ì°¾ìŠµë‹ˆë‹¤.
"""
import os
import sys
from datetime import datetime, timedelta
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv("trendbot.env")

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def debug_naver_news_quality():
    """Naver ë‰´ìŠ¤ ë°ì´í„° í’ˆì§ˆ ë””ë²„ê¹…"""
    print("=" * 60)
    print("ğŸ” NAVER ë‰´ìŠ¤ ë°ì´í„° í’ˆì§ˆ ë””ë²„ê¹…")
    print("=" * 60)
    
    try:
        from src.data_collectors.naver_collector import search_news
        from src.nlp.clean import normalize_for_topics
        
        # í…ŒìŠ¤íŠ¸ í‚¤ì›Œë“œ
        test_keywords = ["ì¸ê³µì§€ëŠ¥", "AI", "ë¨¸ì‹ ëŸ¬ë‹"]
        
        for keyword in test_keywords:
            print(f"\nğŸ“° í‚¤ì›Œë“œ: '{keyword}'")
            
            # ë‰´ìŠ¤ ê²€ìƒ‰
            df = search_news(keyword, display=5)
            
            if df.empty:
                print(f"âŒ '{keyword}'ì— ëŒ€í•œ ë‰´ìŠ¤ ì—†ìŒ")
                continue
            
            print(f"âœ… {len(df)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")
            
            # ê° ë‰´ìŠ¤ì˜ í’ˆì§ˆ ê²€ì¦
            for i, (_, row) in enumerate(df.iterrows()):
                title = row.get('title', '')
                desc = row.get('desc', '')
                
                print(f"\n  ğŸ“„ ë‰´ìŠ¤ {i+1}:")
                print(f"    ì›ë³¸ ì œëª©: {title}")
                print(f"    ì›ë³¸ ì„¤ëª…: {desc[:100]}...")
                
                # í…ìŠ¤íŠ¸ ì •ì œ í…ŒìŠ¤íŠ¸
                combined_text = f"{title} {desc}"
                clean_text = normalize_for_topics(combined_text)
                
                print(f"    ì •ì œëœ í…ìŠ¤íŠ¸: {clean_text}")
                print(f"    ì •ì œ ë¹„ìœ¨: {len(clean_text)/len(combined_text):.2%}")
                
                # ë¬¸ì œ í† í° ê²€ì‚¬
                problematic_tokens = ['nbsp', 'font', 'href', 'https', 'ai', 'rss', 'xml']
                found_problems = [token for token in problematic_tokens if token in clean_text.lower()]
                
                if found_problems:
                    print(f"    âš ï¸ ë¬¸ì œ í† í° ë°œê²¬: {found_problems}")
                else:
                    print(f"    âœ… ë¬¸ì œ í† í° ì—†ìŒ")
        
    except Exception as e:
        print(f"âŒ Naver ë‰´ìŠ¤ ë””ë²„ê¹… ì˜¤ë¥˜: {e}")

def debug_arxiv_quality():
    """arXiv ë…¼ë¬¸ ë°ì´í„° í’ˆì§ˆ ë””ë²„ê¹…"""
    print("=" * 60)
    print("ğŸ” ARXIV ë…¼ë¬¸ ë°ì´í„° í’ˆì§ˆ ë””ë²„ê¹…")
    print("=" * 60)
    
    try:
        from src.data_collectors.arxiv_collector import ArxivCollector
        from src.nlp.clean import normalize_for_topics
        
        collector = ArxivCollector()
        test_keywords = ["machine learning", "artificial intelligence"]
        
        for keyword in test_keywords:
            print(f"\nğŸ“š í‚¤ì›Œë“œ: '{keyword}'")
            
            # ë…¼ë¬¸ ê²€ìƒ‰
            df = collector.collect_papers([keyword], max_results=3)
            
            if df.empty:
                print(f"âŒ '{keyword}'ì— ëŒ€í•œ ë…¼ë¬¸ ì—†ìŒ")
                continue
            
            print(f"âœ… {len(df)}ê°œ ë…¼ë¬¸ ìˆ˜ì§‘")
            
            # ê° ë…¼ë¬¸ì˜ í’ˆì§ˆ ê²€ì¦
            for i, (_, row) in enumerate(df.iterrows()):
                title = row.get('title', '')
                summary = row.get('summary', '')
                text_clean = row.get('text_clean', '')
                
                print(f"\n  ğŸ“„ ë…¼ë¬¸ {i+1}:")
                print(f"    ì œëª©: {title}")
                print(f"    ìš”ì•½: {summary[:100]}...")
                print(f"    ì •ì œëœ í…ìŠ¤íŠ¸: {text_clean[:100]}...")
                print(f"    ì •ì œ ë¹„ìœ¨: {len(text_clean)/len(title + ' ' + summary):.2%}")
                
                # ë¬¸ì œ í† í° ê²€ì‚¬
                problematic_tokens = ['nbsp', 'font', 'href', 'https', 'ai', 'rss', 'xml']
                found_problems = [token for token in problematic_tokens if token in text_clean.lower()]
                
                if found_problems:
                    print(f"    âš ï¸ ë¬¸ì œ í† í° ë°œê²¬: {found_problems}")
                else:
                    print(f"    âœ… ë¬¸ì œ í† í° ì—†ìŒ")
        
    except Exception as e:
        print(f"âŒ arXiv ë””ë²„ê¹… ì˜¤ë¥˜: {e}")

def debug_topic_extraction():
    """í† í”½ ì¶”ì¶œ ê³¼ì • ë””ë²„ê¹…"""
    print("=" * 60)
    print("ğŸ” í† í”½ ì¶”ì¶œ ê³¼ì • ë””ë²„ê¹…")
    print("=" * 60)
    
    try:
        from src.ai.topic_extractor import TopicExtractor
        from src.nlp.clean import normalize_for_topics
        
        # í…ŒìŠ¤íŠ¸ ì½”í¼ìŠ¤ ìƒì„±
        test_corpus = [
            "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì´ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤",
            "ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì„ ì—°êµ¬í•©ë‹ˆë‹¤",
            "ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ê°œë°œí•©ë‹ˆë‹¤",
            "ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ ì„ ì ìš©í•©ë‹ˆë‹¤",
            "ì»´í“¨í„° ë¹„ì „ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤",
            "AI ê¸°ìˆ ì˜ ë°œì „ì´ ê°€ì†í™”ë˜ê³  ìˆìŠµë‹ˆë‹¤",
            "nbsp font href https ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤",
            "RSS XML JSON API ê´€ë ¨ ë‚´ìš©ì…ë‹ˆë‹¤"
        ]
        
        print("ğŸ“Š í…ŒìŠ¤íŠ¸ ì½”í¼ìŠ¤:")
        for i, text in enumerate(test_corpus):
            print(f"  {i+1}: {text}")
        
        # í…ìŠ¤íŠ¸ ì •ì œ í…ŒìŠ¤íŠ¸
        print("\nğŸ§¹ í…ìŠ¤íŠ¸ ì •ì œ ê²°ê³¼:")
        cleaned_corpus = []
        for i, text in enumerate(test_corpus):
            clean_text = normalize_for_topics(text)
            cleaned_corpus.append(clean_text)
            print(f"  {i+1}: {text[:30]}... -> {clean_text}")
        
        # í† í”½ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
        print("\nğŸ¯ í† í”½ ì¶”ì¶œ í…ŒìŠ¤íŠ¸:")
        extractor = TopicExtractor()
        
        # ì›ë³¸ ì½”í¼ìŠ¤ë¡œ í…ŒìŠ¤íŠ¸
        print("\nğŸ“‹ ì›ë³¸ ì½”í¼ìŠ¤ í† í”½ ì¶”ì¶œ:")
        topics_original = extractor.top_topics(test_corpus, k=5)
        print(f"ê²°ê³¼: {topics_original}")
        
        # ì •ì œëœ ì½”í¼ìŠ¤ë¡œ í…ŒìŠ¤íŠ¸
        print("\nğŸ“‹ ì •ì œëœ ì½”í¼ìŠ¤ í† í”½ ì¶”ì¶œ:")
        topics_cleaned = extractor.top_topics(cleaned_corpus, k=5)
        print(f"ê²°ê³¼: {topics_cleaned}")
        
        # ë¬¸ì œ í† í° í™•ì¸
        print("\nğŸš« ë¬¸ì œ í† í° ê²€ì‚¬:")
        problematic_tokens = ['nbsp', 'font', 'href', 'https', 'ai', 'rss', 'xml', 'json', 'api']
        
        for topic_list, name in [(topics_original, "ì›ë³¸"), (topics_cleaned, "ì •ì œ")]:
            found_problems = []
            for topic in topic_list:
                for problem_token in problematic_tokens:
                    if problem_token in topic.lower():
                        found_problems.append(f"{topic}({problem_token})")
            
            if found_problems:
                print(f"  {name}: âš ï¸ ë¬¸ì œ í† í° ë°œê²¬ - {found_problems}")
            else:
                print(f"  {name}: âœ… ë¬¸ì œ í† í° ì—†ìŒ")
        
    except Exception as e:
        print(f"âŒ í† í”½ ì¶”ì¶œ ë””ë²„ê¹… ì˜¤ë¥˜: {e}")

def debug_data_collection_pipeline():
    """ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ ë””ë²„ê¹…"""
    print("=" * 60)
    print("ğŸ” ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ ë””ë²„ê¹…")
    print("=" * 60)
    
    try:
        from src.data_collectors.naver_collector import NaverCollector
        from src.data_collectors.arxiv_collector import ArxivCollector
        from src.ai.topic_extractor import TopicExtractor
        
        # ì„¤ì • ê²€ì¦
        client_id = os.getenv("NAVER_CLIENT_ID")
        client_secret = os.getenv("NAVER_CLIENT_SECRET")
        
        if not client_id or not client_secret:
            print("âŒ Naver API í‚¤ ëˆ„ë½")
            return
        
        print("âœ… í™˜ê²½ ë³€ìˆ˜ ì„¤ì • í™•ì¸")
        
        # ë°ì´í„° ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
        naver_collector = NaverCollector(client_id, client_secret)
        arxiv_collector = ArxivCollector()
        
        test_keywords = ["ì¸ê³µì§€ëŠ¥"]
        
        # ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘
        print(f"\nğŸ“° ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘: {test_keywords}")
        news_data = naver_collector.collect_all_news(test_keywords)
        
        total_news = sum(len(articles) for articles in news_data.values())
        print(f"âœ… ì´ {total_news}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")
        
        # ë…¼ë¬¸ ë°ì´í„° ìˆ˜ì§‘
        print(f"\nğŸ“š ë…¼ë¬¸ ë°ì´í„° ìˆ˜ì§‘: {test_keywords}")
        papers_data = arxiv_collector.search_papers(test_keywords, max_results=5)
        print(f"âœ… {len(papers_data)}ê°œ ë…¼ë¬¸ ìˆ˜ì§‘")
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° í’ˆì§ˆ ê²€ì¦
        print(f"\nğŸ§¹ í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²€ì¦:")
        
        all_texts = []
        
        # ë‰´ìŠ¤ í…ìŠ¤íŠ¸
        for source, articles in news_data.items():
            for article in articles:
                text = f"{article.get('title', '')} {article.get('description', '')}"
                if text.strip():
                    all_texts.append(text)
        
        # ë…¼ë¬¸ í…ìŠ¤íŠ¸
        for paper in papers_data:
            text = f"{paper.get('title', '')} {paper.get('summary', '')}"
            if text.strip():
                all_texts.append(text)
        
        print(f"ğŸ“Š ì´ {len(all_texts)}ê°œ í…ìŠ¤íŠ¸ ìˆ˜ì§‘")
        
        # í† í”½ ì¶”ì¶œ
        if all_texts:
            print(f"\nğŸ¯ í† í”½ ì¶”ì¶œ:")
            extractor = TopicExtractor()
            topics = extractor.top_topics(all_texts, k=10)
            print(f"âœ… {len(topics)}ê°œ í† í”½ ì¶”ì¶œ")
            print(f"ğŸ¯ ì¶”ì¶œëœ í† í”½: {topics}")
            
            # ë¬¸ì œ í† í° ê²€ì‚¬
            problematic_tokens = ['nbsp', 'font', 'href', 'https', 'ai', 'rss', 'xml', 'json', 'api']
            found_problems = []
            for topic in topics:
                for problem_token in problematic_tokens:
                    if problem_token in topic.lower():
                        found_problems.append(f"{topic}({problem_token})")
            
            if found_problems:
                print(f"âš ï¸ ë¬¸ì œ í† í° ë°œê²¬: {found_problems}")
            else:
                print(f"âœ… ë¬¸ì œ í† í° ì—†ìŒ")
        
    except Exception as e:
        print(f"âŒ íŒŒì´í”„ë¼ì¸ ë””ë²„ê¹… ì˜¤ë¥˜: {e}")

def main():
    """ë©”ì¸ ë””ë²„ê¹… í•¨ìˆ˜"""
    print("ğŸš€ ë°ì´í„° í’ˆì§ˆ ë””ë²„ê¹… ì‹œì‘")
    print("=" * 60)
    
    # ê° ë‹¨ê³„ë³„ ë””ë²„ê¹…
    debug_naver_news_quality()
    debug_arxiv_quality()
    debug_topic_extraction()
    debug_data_collection_pipeline()
    
    print("\n" + "=" * 60)
    print("âœ… ë°ì´í„° í’ˆì§ˆ ë””ë²„ê¹… ì™„ë£Œ")
    print("=" * 60)

if __name__ == "__main__":
    main()
